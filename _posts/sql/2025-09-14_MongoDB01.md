### 01. MongoDB

#### MongoDB vs MySQL
- 객체의 이름이 조금 다를 뿐 RDBMS와 비슷한 역할을 한다.
  - 데이터베이스, 컬렉션, 도큐먼트, 필드, 인덱스
- 쿼리 결과를 'Cursor' 로 반환하는 데, 커서를 통해 반복적으로 실제 도큐먼트(레코드)를 가져올 수 있다.
- MongoDB 특징
  - NoSQL
    - 기본적으로 SQL을 사용하지 않지만 MongoDB Connector for BI, Simba의 SQL Driver 등을 사용하면 비슷하게 통신 할 수 있다.
    - 외래키를 명시적으로 지원하지 않지만 $lookup 이라는 Aggregation 기능을 이용하면 조인 처리를 수행(샤딩 제약)할 수 있다.
    - SQL, NoSQL 간의 경계가 허물어지고 있다.
  - Schema-Free
    - 가장 큰 차이점으로 사용할 컬럼을 미리 정의하지 않고 언제든 필요한 시점에 데이터를 저장할 수 있다.
  - 비 관계형 데이터베이스
    - SQL 문법을 지원하지 않고 자바스크립트 기반의 명령을 이용하는 데, JSON 도큐먼트를 인자로 사용

#### MongoDB 배포 형태
- MongoDB 도 HBase, Casandra 같이 클러스터 형태로 서비스할 수 있도록 구현된 DB 서버다.
  - 하지만 꼭 클러스터 형태로 구성해야만 사용할 수 있는 것은 아니다.
- 배포형태는 MySQL 서버의 구조와 매우 비슷하다.
  - 단일 서버로도 사용 가능하며 복제 또는 샤딩된 구조로도 활용 가능하다.
- 단일 노드
  - 해당 배포 형태는 MongoDB는 복제를 위한 로그(OpLog)를 별도로 기록하지 않으며 다른 노드와 통신도 필요하지 않다.
  - 자동 페일오버나 HA 기능이 작동할 수 없어 주로 개발 서버의 구성에 사용
- 단일 레플리카 셋
  - 레플리카 셋 구축을 위해 추가로 MongoDB 서버 필요
  - 장애 상황에서 자동 복구를 위한 최소 단위로 바동 복구가 필요하다면 레플리카 셋으로 MongoDB 배포 필요
  - 노드 간 투표를 통해 Primary 노드를 결정하므로 가능하면 홀수 개의 노드로 구성하는 것이 좋다
  - Arbiter 모드
    - replica set 구성에서 장애 조치와 리더 선출을 위해 투표만 하는 역할을 수행하는 노드 모드입니다.
    - Arbiter의 역할과 특징
      - Arbiter는 데이터 복제를 하지 않고, 오로지 투표 목적으로만 존재합니다.
      - Primary와 Secondary 노드에서 선거(election)가 발생할 때 Arbiter가 투표에 참여해 quorum(과반수)를 채워 줍니다.
      - Arbiter는 직접적인 데이터 저장이나 읽기, 쓰기 연산을 하지 않아 서버 리소스를 거의 소모하지 않습니다.
      - 장애 whcl(failover) 시, Primary 장애 발생 시 Secondary 승격 여부를 결정합니다.
- 샤딩된 클러스터
  - 샤딩된 클러스터에 참여하고 있는 각각의 레플리카 셋을 샤드라고 하는 데, 이 샤드들이 어떤 데이터를 가지는지에 대한 정보는 MongoDB Config 서버 필요
  - 샤딩된 클러스터에서 MongoDB 드라이버는 MongoDB 라우터(mongos)로 연결하고, 자동으로 컨피그 서버로부터 각 샤드가 가지고 있는 데이터에 대한 메타 정보들을 참조하여 쿼리 실행
  - 요청된 쿼리를 실제 데이터를 가지고 있는 샤드로 전달하는 역할 수행, 이후 결과를 정렬 및 병합해 반환

### 02. Storage Engine
<img width="561" height="283" alt="image" src="https://github.com/user-attachments/assets/d8a501a7-6efe-4f51-a50d-29e15b9fed2e" />
  - 출처: https://hoing.io/archives/4561


#### 플러그인 스토리지 엔진
- 스토리지 엔진들은 사용자의 데이터를 디스크에 영구적으로 기록하거나 다시 읽어와서 메모리에 적재하는 역할 담당
  - MongoDB는 MMAPv1, WiredTiger 스토리지 엔진을 선택하여 사용할 수 있고 동시에 사용할 수 없다.
- 옵티마이저라고 부르는 컴포넌트가 처리를 담당하는 데, 최적화된 실행 계획을 수립해준다.

#### MMAPv1 스토리지 엔진
- MMAPv1 특징
  - MongoDB 3.0 까지 주로 사용되던 스토리지 엔진으로 2.6까지는 데이터베이스 단위의 잠금 사용
    - DML 문장의 동시 처리 성능이 좋지 않았는데, 컬렉션 수준 잠금으로 개선되면서 동시성 처리로 변경, 하지만 동시성 처리에 많은 걸림돌이 된다.
  - 내장된 캐시 기능이 없어 운영체제의 캐시 활용
    - 윈도우 캐시를 사용하기 때문에 System call 을 거치게 되어 오버헤드가 상대적으로 크다
- 데이터 파일 구조
  - 데이터 파일은 데이터베이스 단위로 생성된다.
    - storage.smallFiles 옵션을 이용하면 크기를 작게 만들 수 있다. (default 64MB)
  - stage.directoryPerDB 옵션은 디스크의 데이터 파일을 데이터베이스 단위로 별도의 디렉터리에 저장할 것인지 결정
    - true / false
  - database.ns 파일
    - 몽고DB 서버에 생성된 데이터베이스와 컬렉션 그리고 인덱스 정보 저장
  - systemLog.path, processManagement.pidFilePath 옵션을 이용해 생성될 디렉터리

- MongoDB 서버 상태 확인
  - mongostat 도구를 통해 현재 서버의 쿼리 처리량, 메모리 사용량과 같은 전체적인 상태를 볼 수 있다

- 운영체제 캐시
  - 내장된 캐시가 없고 OS에서 제공하는 패이지 캐시 기능에 의존
    - 내장된 캐시가 없기 때문에 더티 페이지를 효율적으로 관리하는 기능이 없다.
    - 많이 발생하는 문제는 Cache Invalidation 현생 (페이지 캐시 삭제)
  - 리눅스 페이지 캐시에 의존하기 때문에 커널의 작동 방식, 파라미터 튜닝이 필수적이며 그 중 디스크의 페이지를 읽어들이는 방식과 페이지 캐시가 갖고 있는 더티 페이지가 디스크를 동기화 하는 방식에 대한 이해가 필수적이다.
  
- 데이터 파일 프레그멘테이션
  - Fragmentation: 도큐먼트의 크기가 계속 증가하면 기존의 작은 공간들은 재활용하지 못하고 계속 빈 공간이 남게 되는 데, 이런 현상이 반복되면 사용하지 못하는 공간이 늘어나게 된다.
  - Fragementation 가 많으면 같은 블록 하나를 읽어도 가져올 수 있는 도큐먼트 개수가 달라지므로 메모리 효율성, 성능 차이가 발생
  - db.collection.stats() 명령어를 통해 확인 가능
    - count: 도큐먼트 건수
    - size: 컬렉션의 전체 도큐먼트 크기
    - storageSize: 컬렉션을 위해 할당된 전체 디스크 데이터 파일 크기
    - avgObjSize: 도큐먼트 하나의 평균 크기
    - paddingFactor: MMAPv1 스토리지 엔진을 사용하여 컬렉션에서만 패딩 사용
  - 디스크 데이터 파일 크기를 줄이기 위해서 컴팩션하거나 세컨드리 멤버의 데이터를 덤프한 후에 다시 적재
    - compaction을 수행하는 경우에 paddingBytes, paddingFactor 옵션 추가 가능

#### WiredTiger 스토리지 엔진
- 특징
  - 내부적인 잠금 경합 최소화(Lock-free algorithm)을 위해 "하자드 포인터", "스킵 리스트" 와 같은 기술 채택
  - MVCC와 데이터 파일 압축, 암호화 기능들을 모두 가지고 있다
- 저장 방식
  - 3가지 타입의 저장소
    - 레코드 스토어: 컬렉션의 레코드를 한번에 저장하는 방식으로 B-Tree 알고리즘 사용
    - 컬럼 스토어: 대용량의 분석 용도로 많이 사용되며 컬럼 단위 또는 컬럼 그룹 단위로 데이터 파일 관리, 데이터 파일 크기가 작아 읽어들이는 속도가 매우 빠르다
    - LSM(Log Structured Merge Tree) 스토어
      - 읽기보단 쓰기에 집중된 저장방식으로 순차 파일 형태로 데이터를 저장하며 시간 순서대로 1개 이상이 관리된다.
      - 메모리에 디스크로 갓 저장된 파일을 Level-0 파일이 되고 조각이 많아지면 이들을 모아 Level-1 데이터 파일 조각을 만든다.
      - 이렇게 n까지 계속 성장하는 방식으로 작동하며 레벨이 높을수록 오래된 데이터로 구성
      - 읽을 떄는 N개의 데이터 파일을 읽어야 하기 때문에 읽기 성능은 떨어지나 대용량의 insert를 문제없이 처리하기 위해 사용
- 데이터 파일 구조
  - MMAPv1 스토리지 엔진과는 다른 디렉터리와 데이터 파일 구조 사용
  - storage.directoryPerDB 옵션을 이용해 데이터베이스별 디렉터리를 구분할 수 있다.
    - 파일의 목록이나 내용에는 차이가 없으며 단지 컬렉션의 데이터 파일이 생성되는 위치가 전용 디렉터리를 사용하는지 아닌지 차이
  - WiredTiger: 텍스트 파일로 현재 실행 중인 WiredTiger 스토리지 엔진 버전 저장
  - storabe.bson: BSON 포맷으로 설정되며 storage.directoryPerDB, storage.directoryForIndexes 옵션 값 저장
  - sizeStorer.wt: 전체 도큐먼트 건수와 각 컬렉션의 데이터 파일 크기 저장
    - 샤딩된 클러스터 환경, 샤드간 데이터 재분산 중, MongoDB 비정상 종료, 고아가 된 다큐먼트가 있을 땐 컬렉션의 도큐먼트 건수를 보여주지 않을 수 있다.
  - WiredTiger.lock: 다른 몽고 서버 인스턴스가 동시에 사용하지 못하도록 잠금 역할하기 위한 파일
    - 재시작되면서 엔진을 초기화할 때 WiredTiger.lock 파일이 남아 있으면 서버가 비정상적으로 종료됐다고 판단, 복구 모드 시작
  - WiredTiger.turtle: 엔진의 설정 내용을 담고 있다
  - WiredTiger.wt: 엔진의 메타 데이터를 저장하는 컬렉션의 데이터 파일
  - _mdb_catalog.wt: 스토리지 엔진 컬렉션과 인덱스의 목록, 메타 데이터를 관리하는 파이
  - WiredTigerLAS.wt: 데이터 페이지들이 더티 페이지 상태여서 디스크에 기록을 해야할 때 해당 파일 사용
    - 캐시에서 재사용할 수 있는 공간이 부족하면 eviction server thread 는 필요한 만큼의 여유 공간을 만들어야 사용자 요청을 원할히 처리할 수 있다.
  - diagnostic.data: 수집된 내부 정보를 1초에 한번씩 모아 별도의 파일로 기록 
- 내부 작동 방식
  - <img width="694" height="529" alt="image" src="https://github.com/user-attachments/assets/123e6c4c-a678-4eb2-b089-6f8ab33f982e" />
    - 출처: https://hoing.io/archives/4561
  - 트랜잭션을 지원하는 RDBMS와 유사한 내부구조로 되어 있다.
  - 다른 DBMS와 동일하게 B-Tree 구조의 데이터 파일과 서버 크래시로부터 데이터 복구하기 위한 저널 로그(WAL)를 가지고 있다.
  - WiredTiger.configString 옵션에 저널 로그와 관련된 설정 변수
    - enabled(저널로그 활성화 여부), archive(체크포인트 이전의 저널로그는 자동 삭제하는 데, 아카이빙하여 보관할 지 여부)
    - file_max(저널 로그 파일의 최대 크기 설정), path(새로운 저널 로그의 디렉터리 경로 설정)
  - 스토리지 엔진 내장된 공유 캐시(버퍼 풀)을 가지고 있다.
    - 쿼리 처리 속도 개선, 쓰기 배치 기능 가능
    - 블록 매니저를 통해 필요한 데이터 블록을 읽어 공유 캐시에 적재하여 쿼리를 처리
- 공유 캐시
  - WiredTiger 스토리지 엔진에서 사용자의 쿼리는 공유 캐시를 거치지 않고 처리할 수 없으며, 하나의 쿼리를 처리하기 위해 수없이 공유 캐시의 데이터 페이지를 참조할 수 있다.
  - 공유 캐시 사용량 확인
    ```
    $ db.serverStatus().wiredTiger.cache."maximum butes configured"
    $ db.serverStatus().wiredTiger.cache."bytes currently in the cache"
    $ db.serverStatus().wiredTiger.cache."tracked dirty bytes in the cache"
    ```
  - 공유 크기 캐시 조정이 가능하며 서버를 재시작하지 않고도 조정할 수 있다.
    ```
    $ db.adminCommand( { setParameter: 1, wiredTigerEngineRuntimeConfig: "cache_size=2G" } )
    ```
  - 디스크의 데이터 페이지를 공유 캐시 메모리에 적재하면서 메모리에 적합한 트리 형태로 재구성하면서 적재
    - 별도의 매핑과정 없이 메모리 주소를 이용해 바로 검색하기 때문에 경합이나 오버헤드가 없다
    - 메모리에 적재하는 과정에서 여러가지 변환 과정을 거치기 때문에 공유 캐시로 읽어오는 과정에서 RDBMS 보다 느리게 처리되지만 적재된 데이터 페이지에서 필요한 레코드를 검색하고 변경하는 작업은 훨씬 효율적으로 동작
  - 하자드 포인터
    - 락-프리(lock-free) 동시성 프로그래밍에서 사용되는 안전한 메모리 회수 기법
    - 여러 스레드가 동시에 접근하는 동적 자료 구조에서, 한 스레드가 노드(객체)를 제거할 때 다른 스레드가 아직 그 노드를 사용하고 있지 않음을 보장
  - 스킵 리스트 (skip-list)
    - lock-free 구현하기 위한 자료구조 중 하나
    - 단순 링크드리스트에서 값을 찾기 위해서는 O(N) 시간복잡도를 가지지만 스킵리스트는 O(log N) 시간복잡도를 가짐
    - B-Tree 보다 성능이 떨어지지만 간단한 구현, 메모리 공간 효율성, lock-free 구현이 가능하다는 장점이 있음
- 캐시 이빅션(Cache Eviction)
  - 공유 캐시를 위하여 지정된 크기의 메모리 공간만 사용하기 때문에 새로운 디스크 페이지를 읽어 적재할 수 있도록 빈 공간을 유지해야 한다
    - 디스크에서 가져오지 못하기 때문에 쿼리 응답 속도가 느려진다
  - 빈 공간을 유지하기 위해 이빅션 모듈을 갖고 있으며 이빅션 서버라 한다.
    - 사용자의 요청을 처리하는 쓰레드와는 별개로 백그라운드 쓰레드로 실행
    - 자주 사용되지 않는 데이터 페이지 위주로 공유 캐시에서 제거하는 작업 진행
  - 최근에는 SSD 속도가 빨라지면서 한번에 읽어들이는 페이지 수가 많아지면서 이빅션 서버에서 제거하는 속도가 읽어오는 속도를 따라가지 못하는 이슈도 있다.
    - 공유 캐시의 여유 공간을 확보하지 못하면 사용자의 쿼리를 처리하는 포그라운드 쓰레드에서 직접 캐시 이빅션 실행

- 체크포인트
  - 커밋된 트랜잭션의 영속성을 보장하기 위해 트랜잭션 로그(WAL, 저널로그)를 먼저 기록하고, 데이터 파일에 기록
  - 체크포인트는 데이터 파일과 트랜잭션 로그가 동기화하는 시점을 의미하며 체크포인트가 실행되어야 오래된 트랜잭션 로그를 삭제하거나 새로운 트랜잭션 로그를 덮어쓸 수 있다.
  - 체크포인트는 복구를 시작할 시점을 결정하는 기준이 된다.
    - 체크포인트 기간이 너무 길면 복구 시간이 길어지게 되고, 너무 빈번히 발생하면 쿼리 처리 능력이 떨어진다.
  - 샤프 체크포인트 (Sharp checkpoint)
    - 평소에는 디스크 쓰기가 많지 않지만, 체크포인트가 실행되는 시점에 한번에 모아 더티 페이지를 기록하는 패턴 
  - 체크포인트 제어 옵션
    - log_size: 얼마나 자주 체크포인트를 실행할 것인지 결정, 설정된 크기만큼 트랜잭션 로그 쓰기가 발생하면 체크포인트 실행 (기본값 0, 스토리지 엔진이 판단)
    - wait: 지정된 시간동안 대기했다가 주기적으로 체크포인트를 실행하도록 설정 (기본값 0, 스토리지 엔진이 판단)
    - name: 체크포인트 이름 설

- MVCC(Multi Version Concurrency Control)
  - 하나의 레코드(다큐먼트)에 대해 여러개의 버전을 동시에 관리하여 필요에 따라 적절한 버전을 사용할 수 있도록 한다
  - 데이터 페이지 영역과 변경 이력(스킵 리스트)이 존재한다
    - update가 실행되면 변경이력에 변경된 데이터를 쌓고 리스트로 관리하며 여러 개의 버전이 관리되도록 유지한다
    - memory_page_max 설정 값보다 큰 메모리를 사용하는 페이지를 찾아 자동으로 디스크에 기록하는 작업(eviction) 수행
    - 이 때, reconciliation 과정을 거치며 변경된 내용이 병합되어 디스크에 기록
  - WiredTiger 스토리지 엔진의 isolation level
    - read uncommitted: 커밋되지 않은 트랜잭션의 변경 내용도 읽을 수 있음
    - read committed: 커밋된 트랜잭션의 변경 내용만 읽을 수 있음
    - snapshot: 트랜잭션이 시작된 시점의 스냅샷을 유지하며, 트랜잭션이 종료될 때까지 동일한 데이터를 읽음 (기본값 - repeatable read와 동일)

- 데이터 블록(페이지)
  - 데이터를 저장하기 위해 고정된 크기의 블록을 사용하지 않고, 가변 크기 블록을 사용
    - 블록의 크기가 너무 커지는 것을 막기 위해 최대 크기에 대한 제한이 있다
  - 컬렉션의 데이터 페이지 크기와 인덱스 페이지 크기를 다르게 설정할 수 있다.
    - collectionConfig.blockCompressor: 컬렉션 데이터 페이지 압축 알고리즘 설정
    - indexConfig.blockCompressor: 인덱스 페이지 압축 알고리즘 설정
    - none, snappy, zlib, zstd (기본값)

- 운영체제 캐시(페이지 캐시)
  - 내장된 공유 캐시를 갖고 있는데 운영체제의 캐시를 경유하는 Cached IO를 기본 옵션으로 사용
    - 더블 버퍼링: 리눅스 커널이 먼저 디스크에 읽어 페이지 캐시에 저장하고 스토리지 엔진이 내장 캐시에 복사
    - 다른 DBMS(주로 RDBMS)는 더블 버퍼링 문제를 해결하기 위해 Direct IO를 사용
  - Wired 스토리지 엔진도 Direct IO를 사용할 수 있도록 제공한다.
    - 더블 버퍼링 이슈는 해결할 수 있지만
    - 쓰기 작업이 디스크에 직접 동기적으로 이루어지므로, 일반적인 Cached I/O 방식보다 쓰기 지연 시간(write latency)이 훨씬 길어질 수 있다.
    - 성능 테스트를 거쳐 Direct I/O를 사용할지 결정해야 한다

- 압축
  - WiredTiger 스토리지 엔진과 RDBMS 비교
    - 가변 사이즈 페이지 사용 가능
    - 데이터 입출력 레이어에서 압축 지원
    - 다양한 압축 알고리즘 지원
  - 가변 크기 페이지를 사용하기 때문에 압축으로 인해 줄어든 만큼의 공간을 절약할 수 있다.
  - 제공하는 압축 형태
    - 블록 압축 / 인덱스 프리픽스 압축 / 사전 압축 / 허프만 인코딩
    - 프리픽스 압축은 블록 압축과 달리 공유 캐시에서도 압축 상태를 유지하며 데이터를 읽을 때 완전한 키 값을 얻기 위해 조립 과정을 거친다
    - 사전 압축과 허프만 인코딩은 디스크와 메모리에서 압축된 형태 사용
  - 압축률이 좋을 수록 인덱스 키를 읽는 속도는 느려질 수 있다.

- 암호화
  - TED(Transparent Encryption for Data) 기능 제공하며 WiredTiger 스토리지 엔진의 압축과 같은 방식으로 동작

### 03. 복제
- 마스터-슬레이브 복제는 초기 사용했던 복제 방식으로 현재는 레플리카 셋 복제로 많은 부분 자동화되었다

#### 복제란(Replication)?
- 여러 서버가 서로의 데이터 동기화를 의미하며 물리적, 논리적 복제로 나눌 수 있다.
  - DRBD(Distributed REplicated Block Device)와 같이 리눅스 서버가 데이터의 내부를 전혀 모르는 상태에서 디스크의 블록만 복제하는 형태가 물리적 방식
  - MySQL, MongoDB 같이 서버간의 데이터를 동기화 하는 방식을 논리적 방식
- Raft 컨세서스 알고리즘
  - 가장 큰 특징은 리더 기반의 복제와 각 멤버 노드가 상태를 가진다는 것
    - 하나의 레플리카 셋에는 반드시 하나의 리더만 존재할 수 있고, 리더는 사용자의 모든 데이터 변경 요청 처리
  - 리더는 요청 내용을 로그에 기록하고 모든 팔로워는 리더의 로그를 가져와 동기화 수행
    - 해당 로그를 OpLog(Operation Log)라 한다
- 복제 목적
  - HA(High Availability: 고가용성)
    - 복제의 가장 큰 목적은 동일한 데이터를 2, 3중 유지하므로써 데이터 손실이 발생해도 데이터를 대체할 수 있게하기 위함
  - 데이터 조회 쿼리 로드 분산
    - 보통 고가용성을 위해 3개의 레플리카로 구성하는 데 조회 쿼리를 분산하기 위해 멤버를 추가할 수 있다.
  - MongoDB는 물리적인 백업 기능을 제공하고 있지 않다.
    - 때문에 긴급하게 데이터 북구해야하는 시점에 secondary 멤버를 멈추고 데이터 파일을 복사해야할 수 있다.
   
#### 레플리카 셋 멤버
- 프라이머리
  - 데이터 변경을 처리할 수 있는 유일한 멤버로 사용자의 변경 요청을 처리하고, 변경된 데이터를 OpLog에 기록하여 다른 멤버들이 동기화
  - 프라이머리가 응답 불가능 상태가 되면 응답을 처리할 수 없게 된다.
    - 세컨더리 멤버들이 하트비트 메세지로 체크하고 있기 때문에 즉시 알아차리고 새로운 프라이머리 선출 투표 진행한다
- 세컨더리
  - 프라이머리로부터 데이터를 실시간으로 가져와 동기화를 하여 동일한 데이터 셋 유지
  - 데이터 변경 요청을 직접 처리할 수 있지만 읽기 요청을 직접 처리하여 부하 분산 용도 가능
  - 데이터 유실 가능성이 있기 때문에 백업 용도로는 어렵다.
- 아비터
  - 아비터는 프라이머리 선출 투표에서 정족수를 채우기 위해 사용

#### 프라이머리 선출
- 레플리카 셋에 현재 프라이머리 멤버가 없으면 프라이머리 선출 진행
  - 해당 레플리카 셋에 프라이마리 멤버가 없으면 사용자 변경 요청을 처리 할수 없다.
  - Read Preference 옵션에 따라서 읽기 쿼리 조차도 불가능할 수도 있다.
- 프라이머리 텀(Term)
  - 프라이머리 선출과정에서 발생하는 논리적시간 (프라이머리가 변경될 때마다 증가하는 고유 식별자)
  - Protocol Version 0
    - MongoDB 3.0 버전까지의 레플리카 셋은 프라이머리 텀(term) 이라는 개념이 없었다.
      - 프라이머리 선출 과정에서 여러 멤버가 동시에 투표를 하면 중복 투표 가능
    - MongoDB 3.2 이전 버전에서는 프라이머리 선출 투표 를 30초에 한 번만 실행될 수 있도록 설계
      - 프라이머리 선출을 위한 투표가 한번 실패하게 되면 그 레플리카 셋은 30초 동안 프라이머리가 없는 상태로 대기
    - 2 단계 투표(Two-Phase Election)
      - 이런 중복 투표나 30초 대기 시간이 최대한 발생하지 프라이머리 선출 시 사전 투표(Speculative-election)와 본 투표(Authoritative-election) 나누어 진행
      - 사전 선거를 통해서 본 선거의 실패 상황(프라이머리를 선출하지 못하는 상황)을 최소화하고자 한 것
  - Protocol Version 1
    - 복잡한 선출 과정을 해결하기 위해서 프라이머리 텀(논리적인 시간의 의미로 사용)이라는 개념 도입
    - Perimary Term이란 투표 식별자이며, 레플리카 셋의 각 멤버들이 프라이머리 선출을 시도할 때마다 1씩 증가하는 논리적 시간
    - 각 멤버들은 투표 요청이 식별자를 기준으로 자기가 투표를 했는지 아니면 다시 투표에 참여하는지 결정 할 수 있다.
    - 단순히 투표를 할 때만 사용되는 것이 아니라 프라이머리 멤버가 사용자의 데이터 변경 요청을 실행한 다음 변경 내용을 OpLog 에 기록할 때마다 현재 텀(Term) 식별자를 같이 기록하여 특정 OpLog가 어느 멤버가 프라이머 였을 때 로그 인지를 식별 가능하게 한다.
   
- 프라이머리 스텝 다운
  - 레플리카 셋 설정된 electionTimeoutMillis 내에 응답이 없다면 레플리카 셋의 각 멤버는 현재 Primary가 없어졌다고 판단
    - 새로운 Primary 선출을 위한 투표를 시작하게 됩니다.
  - 관리자가 의도적으로 프라이머리를 세컨더리로 내리는(Step Down) 작업도 할 수 있다.
    - rs.stepDown() 명령으로 프라이머리를 스텝 다운함
    - rs.reconfig() 명령으로 레클리카 셋 멤버의 우선 순위(Priority) 를 변경함
  - rs.stepDown(stepDownSecs, secondaryCatchUpPeriodSecs)
    - 이 명령이 실행되면 즉시 프라이머리를 내려놓고 stepDownSecs 시간동안 다시 프라미머리가 될 수 없다
    - 이 기간동안 프라이머리가 선출되지 못한다면 해당 프라이머리가 다시 프라이머리가 될 수 있다.
    - secondaryCatchUpPeriodSecs 시간동안 새로운 프라이머리를 선출하지 않고 기다리면서 밀려 있던 복제가 모두 완료(동기화) 되기를 기다린다.
  - rs.config()
    - 명령은 사실 세컨드리를 프라이머리로 전환하는 등의 레플리케이션의 역할(Role) 을 변경하는 직접적인 명령어는 아니다.
    - 하지만 rs.reconfig() 명령으로 레프리카 셋 멤버의 priority 를 변경하여 프라이머리를 세컨더리로 변경할 수 있다.
- 프라이머리 선출 시나리오
  - 프라이머리 선출 트리거 상황
    - 기존 프라이머리 노드가 하드웨어 장애, 네트워크 단절 등으로 인해 더 이상 정상 동작하지 않을 때 선출 발생
    - 프라이머리가 현재 셋에 존재하지 않는다면, 세컨더리 노드가 이를 감지하여 즉시 투표 프로세스를 개시한다.
  - 선출 방식과 과정
    - MongoDB는 Self-Election(자기 선출) 방식을 채택하고 있다.
    - 각 세컨더리 노드는 프라이머리가 필요하다는 사실을 인지하면 자기 자신을 후보로 하여 투표 과정을 시작한다.
  - 투표 조건
    - 후보 노드가 같은 레플리카 셋 소속인지
    - 후보 노드의 우선순위(priority)가 가장 높은지
    - 요청한 선출 Term(term)이 현재 자신이 아는 값보다 최신인지
    - 기존에 해당 Term에 투표한 적이 없는지
    - 데이터 동기화 상태(OpTime)가 가장 최신 또는 동등한지
  - 투표 및 정족수
    - 과반수 이상의 투표를 받아야 프라이머리로 선출된다.
    - 멤버의 votes 값이 1이어야만 정족수에 포함된다. Non-Voting 멤버는 투표 인원 산정에서 제외된다.
    - 스플릿 브레인(Split-brain) 방지: 과반수 연결이 안 되면 기존 프라이머리도 세컨더리로 자동 강등된다.
  - 기타 고려 사항
    - 후보보다 더 우선순위가 높거나, 더 최신 데이터를 가진 멤버가 있으면 투표가 거부된다.
    - 프라이머리 선출 실패 시, 지체 없이 새로운 투표가 다시 시작될 수 있다(Protocol Version 1).
    - 복구 과정이나 동기화 상태(STARTUP 등)에 따라 투표에 참여하지 못하는 멤버도 생길 수 있다.
- 롤백 및 롤백 데이터 재처리
  - 롤백 발생 원인 및 과정
    - 프라이머리에서 작성된 데이터가 세컨더리로 복제되기 전에 프라이머리 장애 또는 선출(Primary election)이 발생하는 경우
    - 해당 시점 이후 프라이머리에 기록된 데이터가 세컨더리에는 적용되지 않는다.
    - 장애 후, 해당 노드(이전 프라이머리)가 세컨더리로 다시 합류하면, 자신만 가지고 있는(다른 노드와 공유되지 않은) OpLog를 기준으로 롤백 과정을 진행한다.
    - 이 과정에서 새로운 프라이머리의 OpLog를 기준으로 자신과 일치하지 않는 트랜잭션 및 쓰기를 삭제(되돌리기)한다.
    - 즉, Primary만 갖는 데이터를 동기화하는 것이 아닌 삭제하는 작업
  - 롤백의 내부 동작
    - 이전 프라이머리였던 멤버는 자신의 OpLog를 역순으로 검사하여 공통되는 지점을 찾고, 그 이후의 OpLog를 삭제
    - 롤백 과정에서 OpLog만 삭제하는 것이 아니라, 관련 실제 데이터도 컬렉션에서 제거 또는 원래 상태로 복원
    - 롤백으로 인해 삭제‧변경된 데이터들은 서버의 데이터 디렉터리 하위 rollback 폴더에 BSON 파일로 저장된다.
    - 필요 시 bsondump 유틸리티로 변환하여 수동 복구가 가능
  - 롤백 한계 및 수동 재처리
    - 롤백 데이터의 크기가 300MB를 넘으면 자동 롤백은 실패하며, 이 경우 에러 로그가 남고 수동 복구가 필요하다.
    - 수동 재처리 시 rollback 디렉터리의 데이터를 관리자 또는 개발자가 직접 검토 및 복원해야 한다.
      - bsondump 유틸리티로 해당 BSON 파일의 내용을 확인하거나 JSON 형식으로 변환할 수 있다.
        - bsondump <파일명>.bson > <파일명>.json
      - 복구 대상 데이터가 확실하다면 mongorestore 명령어로 롤백된 BSON 파일을 원하는 데이터베이스 또는 컬렉션으로 복구
        - mongorestore --host <dbhost> --port 27017 --db <database> --collection <collectionName> <파일경로>/<파일명>.bson
      - 필요한 경우 --drop 옵션을 추가해 기존 데이터베이스나 컬렉션을 덮어쓸 수도 있다.
  - 트랜잭션 내의 롤백과 차이
    - 트랜잭션 롤백은 커밋되지 않은 변경 사항이 취소되는 메커니즘
    - 레플리카 셋의 롤백은 복제 불일치로 인한 데이터 정합성을 맞추기 위해 이미 기록된 데이터를 되돌린다는 점에서 다르다.

#### 복제 아키텍처
- 실시간으로 변경되는 데이터는 세컨드리 멤버들이 프라이머리의 OpLog를 가져온 다음 재생하면서 동기화
    - 초기 동기화(Initial Sync), 실시간 복제 두 단계를 나누어 생각할 수 있다.
- 복제로그 (OpLog) 구조
  - MongoDB는 복제 로그를 데이터베이스 서버의 "oplog.rs" 라는 이름의 테이블(컬렉션)으로 기록
    ```
    $ db.oplog.rs.find().pretty()
    ```
    - ts(Timestamp): 저장 순서를 결정하는 기준으로 동기화를 잠깐 멈추거나 재시작할 때 기준이 된다
    - t(Primary Term): ts와 동일하게 증가하는 값으로 레플리카 셋의 프라이머리를 선출하는 투표가 실행될 때마다 증가하는 값
    - h(Hash): OpLog 레코드의 해시값으로 식별자 역할을 한다
    - v(Version): 도큐먼트 버전
    - op(Operation Type): 수행된 작업의 유형을 문자로 표현 (예: i-insert, u-update, d-delete, c-command, n-no operation(단순 정보성 메세지 의미))
    - ns(Namespace): 변경된 대상의 네임스페이스 (데이터베이스 이름과 컬렉션 이름의 조합) 저장
    - o(Operation): 실제 변경된 정보 저장
    - o2(Operation2): 업데이트 작업의 경우 변경된 필드와 값이 저장
- local 데이터베이스
  - MongoDB는 서버 자신을 위한 local 이란 이름의 컬렉션 생성
    - replica set 멤버들이 서로의 상태를 파악하고 동기화하는 데 필요한 메타데이터를 저장
    - 모든 복제 세트 멤버들은 local.oplog.rs 컬렉션에 복제 대상이 되는 모든 쓰기 작업(삽입, 수정, 삭제) 기록
      - 보조(secondary) 멤버들은 이 OpLog를 읽어서 프라이머리 멤버와 동일한 데이터 상태를 유지
      - 로그는 롤링 방식으로 관리되며, 일정 크기 이상이 되면 오래된 데이터부터 삭제됩니다. 
    - local.replset과 같은 컬렉션에 복제 세트의 구성, 멤버들의 상태, 투표(elections) 정보 등을 저장합니다
- 초기 동기화 (Initial Sync)
  - 레플리카셋에 투입하면 MongoDB는 이미 투입돼 있던 멤버로부터 모든 데이터를 일괄로 가져오는 과정 (초기 동기화)
    - 해당 과정은 투입되는 멤버의 디렉터리가 완전히 비어있는 경우 초기 동기화를 실행하고 아닌 경우 스킵한다
  - 주의 사항
    - 단일 쓰레드로 진행하기 때문에 많은 시간 소요
    - 중간에 멈췄다가 다시 시작하면 처음부터 다시 시작해야 한다
  - 수동 초기 동기화 (부트스트랩)
    - 정상적인 멤버의 데이터 파일을 그대로 복사하여 새 멤버에 붙여넣는 방법
  - 자동 초기 동기화
    - MongoDB 서버가 자동으로 다른 멤버로부터 데이터 베이스를 복사하는 방법으로 별도의 오퍼레이션 없이 자동으로 진행
    - 복사 과정
      - 데이터 베이스 복제(clone)
      - OpLog를 이용한 일시적인 데이터 동기화
      - 인덱스 생성
- 실시간 복제
  - 복제 아키텍처
    - 쓰기 작업 기록
      - 클라이언트가 프라이머리 노드에 데이터를 쓰면, MongoDB는 해당 작업을 먼저 프라이머리 노드의 local.oplog.rs 컬렉션에 기록
      - oplog는 캡드 컬렉션(Capped Collection)으로 구현되어 있어, 크기가 일정 수준을 넘으면 가장 오래된 데이터가 자동 삭제
      - 캡드 컬렉션의 가장 큰 특징은 테일러블 커서 기능으로 데이터가 추가될 때마다 해당 커서를 통해 최신 데이터를 보내주는 형태로 동작
    - 세컨더리 동기화
      - 세컨더리 노드들은 백그라운드 스레드(Observer)를 통해 프라이머리의 oplog를 지속적으로 폴링(polling)하며 아직 적용되지 않은 새로운 작업을 발견하면 로컬 MongoDB의 메모리 큐에 저장
      - Replication Batcher는 큐에서 일정 개수를 가져와 이를 자신의 데이터에 순차 적용하여 데이터 정합성 유지
        - 최종 일관성
    - 장애 조치(Failover) 및 프라이머리 선출
      - 프라이머리 노드가 네트워크 단절이나 장애로 인해 다운되면, 복제 세트의 남은 멤버들은 투표(election) 진행
      - 가장 최신 데이터를 가진 세컨더리 노드가 새로운 프라이머리로 선출되어 쓰기 작업을 진행하며 애플리케이션의 다운타임을 최소화합니다.

#### 복제로그 설정
- OpLog 컬렉션 크기 설정
  - oplog.rs 컬렉션의 얼마나 담을 수 있느냐에 따라 세컨드리가 허용 가능한 지연시간이 결정된다
  - 기본값은 5%이며, 최소 1GB 이상으로 설정하는 것이 좋으며 oplogsizeMB 옵션을 이용해 명시적으로 설정할 수 있다.
  - db.getReplicationInfo() 명령어로 현재 OpLog 크기와 보관 기간을 확인할 수 있다.
- 복제 동기화 상태 확인
  - 어디까지 동기화했는지 또는 복제가 지연이 발생하고 있는지 rs.printSlaveReplicationInfo() 명령을 통해 확인할 수 있다
  - 모든 동기화됐는지 확인이 필요하면, 실제 oplog.rs 컬렉션의 최신 도큐먼트를 조회해 ts 값 확인
    ```
    use local
    db.oplog.rs.find().sort({$natural:-1}).limit(1).pretty()
    ```
- OpLog 컬렉션과 백업
  - PITR (Point-in-Time Recovery)
    - PITR은 특정 시점으로 데이터를 복원하는 기술로 oplog는 이 기술의 핵심 요소가 된다 
    - 백업 방식: 먼저, 특정 시점의 완전 백업(Full Backup) 수행, 이 백업에는 oplog의 특정 시점(예: 백업 시작 시점)까지의 데이터가 포함된다 
    - 복원 과정: 완전 백업 데이터를 사용하여 복구할 서버에 기본 데이터를 복원 
      - 백업 시점부터 복원하고자 하는 특정 시점까지의 oplog를 순차적으로 적용(replay)하여 데이터를 동기화합니다. 
    - Oplog 역할: oplog는 백업 시점 이후 발생한 모든 변경 이력(insert, update, delete)을 담고 있어, 백업 데이터를 최신 상태 또는 원하는 시점까지 업데이트
      - 이를 통해 데이터 손실을 최소화하고 매우 정밀한 복원이 가능합니다.
  - 증분 백업 (Incremental Backup)
    - 증분 백업은 마지막 백업 이후 변경된 데이터만 백업하는 방식으로 oplog를 사용하여 이를 구현한다. 
    - 백업 방식: 첫 번째 완전 백업을 수행한 후 마지막 백업 시점 이후에 발생한 oplog 레코드만 추출하여 백업 
    - Oplog의 역할: oplog는 변경 이력을 기록하므로, oplog를 주기적으로 백업하는 것은 곧 증분 백업을 수행하는 것과 동일하다 
    - 복원 과정: 가장 최근의 완전 백업을 복원. 완전 백업 이후에 백업된 모든 oplog 백업 파일을 순서대로 적용.

#### 레플리카 셋 설정
- 하트비트 메세지 주기와 프라이머리 선출 타임아웃
  - protocolVersion 1 부터는 electionTimeoutMillis 옵션을 통해 프라이머리 선출 타임아웃 설정 가능
    - 기본값은 10초이며, 네트워크 환경이 불안정한 경우 이 값을 늘려주는 것이 좋다
  - hearbeatIntervalMillis, heartbeatTimeoutSecs 옵션을 통해 하트비트 주기와 타임아웃 설정 가능
    - 기본값은 각각 2초, 10초이며, 네트워크 환경이 불안정한 경우 이 값을 늘려주는 것이 좋다
  - 해당 설정을 통해 프라이머리 선출이 늦어질 수 있지만 네트워크 영향에 덜 민감하며 반대로 프라이머리 선출을 빠르게 할 수록 네트워크 영향에 민감하다
- 레플리카 셋 멤버 설정
  - 멤버 우선순위
    - 레플리카 셋의 priority 값을 명시적으로 1이 아닌 값을 재설정하는 방법은 불필요한 투표가 발생할 수 있기 때문에 추천하지 않는다
  - 투표권
    - 레플리카 셋의 멤버가 갖는 투표권의 개수를 의미
    - 3.0 이전에는 투표권 2개 이상 설정이 가능했으나, 3.0부터는 모든 레플리카가 0 또는 1개의 투표권을 갖는다
    - 최대 7개까지 허용되므로 모든 멤버가 투표권을 갖을 수 없다
  - 히든 멤버
    - 레플리카 셋 내에서 각 멤버는 각 용도별로 활용해야할 수 있는데, 백업이나 복구 같은 관리자 작업을 위해 클라이언트로부터 숨길 수 있는 기능 제공
      - 클라이언트에게 노출되지 않아 쿼리를 처리하진 않지만 복제 로그를 가져와 항상 최신 데이터를 갖는 형태
    - hidden 옵션을 true로 설정하면 해당 멤버는 클라이언트로부터 숨겨지며, 우선순위(priority)도 0으로 설정되어 프라이머리로 선출되지 않는다
  - 지연된 복제
    - 사용자나 관리자의 실수로 데이터가 손상됐을 때 해결책을 제공하진 않는다. 그래서 mongo는 이런 실수로부터 최소한으로 보호하기 위해 지연된 복제 기능 제공
    - settings.slaveDelay: 3600초로 설정하면 해당 멤버는 프라이머리로부터 1시간 늦게 복제 로그를 가져와 적용

#### 레플리카 셋 배포
- 레플리카 셋 멤버의 수
  - 투표 가능한 최대 멤버 수
    - 레플리카셋 멤버는 최대 50개까지 가능하지만, 투표 가능한 멤버는 최대 7개까지만 허용
    - 따라서, 7개 이상의 멤버를 구성하려면 일부 멤버는 투표권을 갖지 않도록 설정해야 한다
  - 홀수 멤버 유지
    - 프라이머리 선출은 반드시 과반수 이상의 멤버가 투표에 참여할 수 있어야 하는데, 멤버 수가 짝수이면 과반수 확보가 어려울 수 있다
    - 반드시 3개 이상의 멤버로 레플리카 셋을 구성하는 것이 좋다, 필요 시 Arbiter를 추가하여 홀수 멤버로 유지
  - 읽기 쿼리 분산
    - 레플리카셋 멤버를 추가하는 이유는 읽기 쿼리를 분산하기 위한 것
  - 레플리카 셋의 멤버 확장을 위한 여분의 멤버
    - Arbiter를 추가했을 때에 장점은 서버 비용을 절감할 수 있느 것이다.
    - 단점으로는 백업을 위한 멤버가 부족한 것 이다.
  - 레플리카 셋의 이름
    - 레플리카 셋의 이름은 컨넥션 식별자로 사용하기 때문에 유니크하게 부여하는 것이 좋다
- DR(Disaster Recovery) 구성
  - 하나의 IDC가 사용할 수 없는 상황에서 다른 IDC에 있는 멤버가 프라이머리로 선출되어 서비스를 지속할 수 있도록 구성
  - 3개 멤버로 구성된 레플리카 셋
    - A zone에 2개 멤버(primary), B zone에 1개 멤버로 구성
      - A zone이 사용 불가능해지면 B zone의 멤버가 프라이머리로 선출이 불가능해진다
    - A zone에 1개 멤버(primary), B zone에 1개 멤버, C zone에 1개 멤버로 구성
      - B, C zone 중에서 사용불가능하게 되어도 프라이머리 역할을 계속 수행 가능
      - A zone이 사용 불가능해지면 B, C zone 중에서 하나가 프라이머리로 선출되어 서비스를 지속할 수 있다 
- 레플리카 셋 배포 시 주의사항
  - 세컨드리 멤버의 장비 사양
    - 프라이머리와 세컨드리 멤버에 대해서 서버의 사양을 동일하게 투입하는 것이 일반적
    - 세컨드리 멤버는 프라이머리의 OpLog를 읽고 자신의 데이터에 적용하는 작업을 수행하기 때문에 CPU와 메모리 사양이 너무 낮으면 복제 지연이 발생할 수 있다
  - 레플리카 셋 멤버의 네트워크 분산
    - 서버 구성에서 자주 일어나는 문제는 내장된 HDD
      - 그래서 대부분 디스크 드라이브는 RAID, 이중화를 기본으로 구축하는 경우가 많다
    - 그다음 네트워크 장비를 통한 장애
      - IDC 내에서 MongoDB 레플리카셋을 구축할 때 동일한 네트워크 스위치로 연결하게 되면 네트워크 스위치 장애 시 모든 멤버가 동시에 다운되는 상황이 발생할 수 있다
      - 다른 네트워크 스위치로 연결되게 서버를 배포하는 것은 필수 사항
     
### 04. 샤딩
#### 샤딩이란
- 샤딩이란
  - 샤딩은 데이터를 여러 서버에 분산하여 저장하고 처리할 수 있도록 하여 고가용성과 대용량 분산 처리를 위해 복제와 샤딩 모두 사용
  - 샤드 클러스터를 구축해야 하는 데, 샤드 위치 정보 등 메타 정보를 저장하기 위한 컨피그 서버가 필요하며 mongos(프록시 역할) 라는 라우팅 서버 필요
- 샤딩의 필요성
  - 데이터베이스 서버를 확장하려면 데이터베이스의 데이터가 여러 서버로 분산될 수 있게 한다
  - 스케일 업에 경우 빠르게 한계에 이를 수 있다.
- 샤딩의 종류
  - 샤딩은 데이터를 어떤 형태로 파티션할지에 따라 수평, 수직 샤딩으로 구분하는 데, 수평 또는 수직 파티션이라고 한다
    - 수직 샤딩: 기능별로 컬렉션을 그룹핑하여 그룹벼로 샤드를 할당하는 방식으로 일반적으로 초기에 간단히 샤딩이 필요한 경우 사용
    - 수평 샤딩: 도큐먼트를 영역별로 파티셔닝하여 1/N개씩 각 샤드가 나눠 가지는 방식
    - 수평샤딩은 파티셔닝의 기준이 되는 필드(샤딩키) 선정이 매우 중요하며 이에 따라 각 샤드의 부하가 균등해질 수 있고 아닐 수 있다
  - 처음에는 샤드 클러스터 없이 서비스를 진행하다 사용량이 늘어나면 수직 샤딩 방식을 고려하여 컬렉션 단위로 묶어 나누어 구축
  - 이후 사용자가 더 늘어 데이터가 기하급수적으로 늘어나면 수평 샤딩을 조금씩 적용하게 되어 DB 구성이 진화된다
  - MySQL과 MongoDB
    - MySQL은 트랜잭션 관리가 필요할 때 활용하는 것이 좋다

#### 샤딩 아키텍처
- 샤드 클러스터를 구성하기 위해서 가장 중요한 3가지 컴포넌트는 샤드 서버, 컨피그 서버, 라우터(mongos)이다
- 샤드 클러스터 컴포넌트
  - 라우터는 영구적인 데이터를 가지지 않고, 쿼리 요청을 어떤 샤드로 전달할지 정하고, 쿼리 결과 데이터를 병합하여 리턴한다
  - 컨피그 서버는 영구적으로 데이터를 저장하는 데, 샤드 서버는 실제 사용자의 데이터를 저장하는 반면 컨피그 서버는 샤드 서버에 저장된 사용자 데이터가 어떻게 나누어 분산돼 있는지 메타 정보 저장
- 샤드 클러스의 쿼리 수행 절차
  - 컨피그 서버는 샤딩이 활성화된 DB와 컬렉션 정보를 관리
    - 샤드 클러스터에 컬렉션을 생성해도 샤딩이 되지 않은 객체들은 컨피그 서버가 아니라 각 샤드 서버가 로컬로 관리
  - 라우터의 쿼리 절차
    - 사용자 쿼리가 참조하는 컬렉션의 청크 메타 정보를 컨피그 서버로부터 가져와 라투어 메모리에 캐싱
    - 사용자 쿼리의 조건에 샤딩 키 조건을 찾음
      - 쿼리 조건에 샤딩 키가 있으면 청크 정보를 라우터의 캐시에서 검색하여 해당 샤드 서버로만 사용자 쿼리 요청
      - 샤딩 키 조건에 포함된 청크가 여러 샤드에 걸쳐있다면 대상이 되는 여러 샤드 서버에 쿼리 요청
      - 쿼리 조건에 샤딩 키가 없으면 모든 샤드 서버로 사용자의 쿼리 요청
    - 쿼리를 전송한 대상 샤드 서버로부터 쿼리 결과가 도착하면 결과를 병합하여 사용자에게 쿼리 결과 반환
- 컨피그 서버: 샤딩된 클러스터를 운영하는 데 있어 필요한 정보 저장
  - databases
    - _id(DB 이름 저장), partitioned(샤딩 활성화 여부), primary(프라이머리 샤드가 어느 샤드인 지)
  - collections
    - 샤드 클러스터가 가지고 있는 컬렉션 목록 관리
    - _id(db, collection 이름으로 구성), lastmod(마지막 변경 시점), dropped(삭제된 컬렉션 여부), key(샤딩 키), unique(샤딩키의 unique 여부), lastmodEpoch(컬렉션의 마지막 변경 일시를 ObjectId 포맷으로 저장)
  - chunks
    - 샤딩된 컬렉션의 모든 청크 정보 관리
    - _id(청크 아이디 저장), lastmod(마지막 변경 시점), lastmodEpoch(마지막 변경 일시를 ObjectId 포맷으로 저장), ns(네임스페이스), min(청크 시작 값), max(청크 종료 값), shard(저장된 샤드의 이름 저장)
  - shards
    - 샤드 크럴스터에 등록된 모든 샤드 서버의 정보를 레플리카 셋 단위로 관리
  - mongos
    - 라우터를 파악할 수 있도록 30초 단위로 샤드 클러스터의 모든 멤버와 ping을 주고 받는다
    - mongos에는 현재 컨피그 서버와 연결했던 모든 mongos 목록 저장하며 삭제하지 않는다
  - settings
    - 청크의 밸런싱과 관련된 작업의 설정 저장
  - version
    - 컨피그 서버가 갖고 있는 샤드 클러스터의 메타 데이터 전체에 대한 버전 정보 저장
  - lockpins
    - 컨피그 서버와의 연결을 계속해서 확인하는 데, 어떤 멤버가 언제 연결 상태를 확인했는지 저장
  - locks
    - 샤드 서버나 라우터 멤버들이 서로의 작업을 동기화하면서 처리하는 데, 작업을 동시에 시작하면 충돌이 발생할 수 있다.
    - locks 컬렉션을 활용하여 동기화하여 처리
  - changelog
    - 컨피그 서버의 메타 정보 변경을 유발한 이벤트에 관해 정보성 이력 관리
- 컨피그 서버 복제 방식
  - 메타 정보는 사용자 데이터의 일관성을 유지하기 위한 중요한 정보로 3대 이상으로 복제할 것을 권장하고 있다
  - SCCC(Sync Cluster Connection Config)
    - 각 config 서버가 완전히 독립적으로 동작, mongos(라우터)가 3대의 config 서버에 모두 직접 연결해 데이터를 동기화
    - 라우터가 데이터를 갱신할 때 3대의 config 서버 모두에 UPDATE 쿼리를 따로 보내고, 분산 트랜잭션 사용
    - 데이터 불일치, 동기화 문제, 구현 복잡성 등 여러 안정성 문제가 있어 MongoDB 3.2 이전에만 사용됩니다.
  - CSRS(Config Server as Replica Sets)
    - config 서버도 샤드 데이터처럼 레플리카 셋(복제본 세트)으로 구성하는 방식
    - 최소 3대 이상으로 구성이 필요하고, 아비터(Arbiter)나 지연 멤버는 사용할 수 없다
    - config 서버끼리 자동으로 데이터 복제를 수행하며, 장애 발생 및 일관성을 훨씬 효과적으로 보장한다
    - MongoDB 3.4 이상에서는 이 방식만 지원하며, WiredTiger 스토리지 엔진을 반드시 사용해야 한다
- 컨피그 서버 가용성과 쿼리 실행
  - 컨피그 서버를 레플리카셋으로 구성할 때, 멤버의 수는 구성 방식에 따라 다르다
    - SCCC 방식은 1, 3 대로 상숫값으로 관리, CSRS 방식은 일반적인 레플리카 셋 구성에 필요한 멤버의 수와 같다 
  - ReadConcern, WriteConcern 을 majority 설정
    - 레플리카 셋 멤버의 과반수에 접근할 수 있어야 쿼리를 수행할 수 있다
  - mongos는 처음 기동될 때 컨피그 서버의 메타 정보를 일괄적으로 로드해 캐시 메모리에 적재한다
    - 라우터 서버가 컨피그 서버에 데이터 변경 쿼리 실행한다
    - 청크 마이그레이션 실행, 청크 스플릿 실행할 때 컨피그 서버의 데이터 변경
    - 라우터 서버가 새로 시작되는 경우, 컨피그 서버의 메타 데이터가 변경될 때, 사용자 인증 처리 시 컨피그 서버에서 조회
  - 컨피그 서버가 응답 불능 상태일 때, mongos에 메모리에 적재된 데이터로 정상적으로 서비스가 가능하나 조회가 필요할 땐 불가능한 상황이 된다
- 라우터
  - 사용자의 쿼리 요청을 샤드 서버로 전달하고, 쿼리결과를 모아 반환하는 프록시 역할을 하며 샤드간 청크 밸런싱 및 청크 스플릿 수행
    - 청크: 컬렉션을 샤드 키를 기준으로 나눈 파티션 단위이며, 각 청크는 여러 샤드 서버에 분산 저장
    - 청크 밸런싱: 청크 개수의 불균형이 생기지 않도록, 운영 중 밸런서 프로세스가 청크를 필요에 따라 자동으로 다른 샤드로 이동
    - 청크 스플릿: 데이터가 계속 쌓여서 크기가 너무 커지면, 밸런서가 자동 또는 수동으로 청크를 둘로 나누는 작업을 수행
  - 라우터는 클러스터의 메타 정보를 메모리에 캐시하고 있어 어느 서버로 전달해야할 지 판단
  - 정렬이나 limit, skip 과 같은 쿼리 옵션을 처리
    - 정렬: 샤드 서버들중에 프라이뭐리 샤드를 결정하고 프라이머리 샤드는 쿼리 결과를 전달받아 먼저 정렬을 수행한 후 라우터 서버로 반환
    - limit 옵션을 샤드로 전달하여 결과를 다시 limit 처리하여 수행해 결과 반환
    - skip 옵션이 있는 경우 샤드 서버로 쿼리를 전달하고 mongos 내에서 skip 적용
- 라우터의 쿼리 분산
  - 타겟 쿼리
    - 샤드 키 조건을 가진 사용자 쿼리에 대해 라우터는 조건을 분석해 원하는 데이터만 있는 특정 샤드로만 요청하는 형태
    - 샤드키가 단일 필드일 때: 쿼리 조건에 해당 필드가 정확한 값(혹은 특정 범위)으로 포함되어 있으면 타겟 쿼리가 된다.
      - 예시: db.coll.find({userId: "abc123"}) 샤드키가 userId일 경우, 해당 값에만 해당하는 샤드로만 쿼리가 전달된다.
    - 컴파운드 샤드키(2개 이상의 필드 조합)일 때: 반드시 선행 필드가 조건절에 포함되어 있을 때 타겟 쿼리가 가능하다.
      - 예시: 샤드키가 (date, time)이라면
      - {date: "2025-01-01", time: {"$gte": "00:00:00", "$lt": "13:00:00"}} → 타겟 쿼리
      - {date: "2025-01-01"} → 타겟 쿼리(날짜 기준으로 충분히 범위를 좁힘)
      - {time: {"$gte": "00:00:00"}} → 타겟 쿼리 불가(선행 필드 date 없음)
  - 브로드캐스트 쿼리
    - 샤드 키를 쿼리 조건으로 가지지 않는경우 모든 샤드 서버로부터 요청하는 경우
    - 샤드키와 무관하게 다중 업데이트, updateMany, deleteMany 와 같은 쿼리도 브로드캐스트 쿼리가 발생한다 
- 라우터 배포
  - 라우터는 N개 이상의 클라이언트와 연결을 맺고, 동시에 N개 이상의 샤드 서버와 연결을 맺는 서버
  - 응용 프로그램 서버와 함께 라우터 배포
    - 각 애플리케이션 서버에 mongos를 로컬로 설치해, 애플리케이션에서 바로 mongos로 쿼리를 전달.
    - 장점: 네트워크 지연(minimum round-trip latency) 최소화, 부하분산에 유리.
    - 단점: mongos 인스턴스 수가 늘어 관리 복잡성 증가. 장애 발생 시 대응이 어려우며, 샤드 서버와의 커넥션 수가 급증.
  - 전용의 라우터 서버 배포
    - 애플리케이션은 하나 이상의 독립 mongos 서버 목록에 접속, 모든 라우팅을 중앙화.
    - 장점: 운영 관리가 용이하며, 커넥션을 집중 관리 가능하고 샤드 서버와의 커넥셔 수를 절감한다
    - 단점: 라우터 서버 장애 시 전체 서비스 영향이 큼. 부하 분산 및 failover 설계, 드라이버 설정에 라우터 주소 명시해야 함.
  - L4와 함께 라우터 배포
    - 여러 mongos 라우터를 L4 로드밸런서 뒤에 배치해, 애플리케이션이 L4 주소로 접속.
    - 장점: 라우터 장애시 자연스럽게 트래픽이 healthy mongos로 분산(고가용성), mongos 주소 직접 설정 불필요.
    - 단점: 네트워크 hop 증가로 응답 지연 가능
      - 커서 기반 read(getMore) 요청이 항상 동일 mongos로 전달되지 않으면 에러 발생 가능,스티키 세션 설정 필요.
  - 샤드 서버/컨피그 서버와 함께 라우터 배포
    - mongos를 샤드 서버, 또는 config 서버와 병행 설치, 하나의 물리서버에 여러 역할.
    - 장점: 서버 자원 효율적으로 사용, 실환경에서 Atlas 등에서 활용됨. 관리자용, 밸런싱 등 특수 목적 mongos 분리 가능.
    - 단점: 데이터 트래픽과 라우팅 트래픽 동시 발생시 네트워크, I/O가 몰릴 수 있음. 컨피그 서버 부하 우려.

#### 샤딩 알고리즘
- 청크
  - 여러 샤드에 분산되어 관리되는 데, 컬렉션 조각(파티션)을 청크라고 하고 샤드 키의 원본값, 해시값의 범위(minKey,maxKey) 존재
  - 청크 최소/대 값은 균등하게 나눌 필요 없으며 규칙적으로 생성되지 않는다
  - 샤드 간 청크 개수의 불균형이 발생하면 청크가 다른 샤드로 옮겨지고 실제 insert/delete 가 이뤄진다
  - 청크는 기본적으로 64MB 까지 커질 수 있고, 이상 커지면 밸런서에 의해 split 된다
- 레인지 샤딩
  - 샤드 키의 값을 기준으로 범위를 나누고, 사용자 데이터가 어느 청크에 포함될지 결정하는 샤딩 알고리즘 (minKey, maxKey)
  - 청크의 크기가 커지면 자동 분할되고, 밸런서가 청크의 분포를 균형 있게 유지하도록 다른 샤드로 이동한다.
  - 장점
    - 범위검색 쿼리(range query) 처리시 성능이 매우 뛰어나고, 타겟 쿼리 구현에 유리
    - 쿼리 조건에 샤드 키의 범위를 명시하면 특정 샤드만 타겟팅 가능해 효율적 분산 처리.
  - 단점
    - 데이터 쏠림(편향)이 발생하기 쉽다: 특정 범위나 한쪽에 데이터가 몰리면 샤드 간 자원 불균형이 커질 수 있음.
    - 매우 편향된 샤드 키(예: 최신날짜에만 삽입 집중)일 경우 점보 청크/jumbo chunk 문제 및 성능 저하 발생.
- 해시 샤딩
  - 샤드 키값에 해시 함수를 적용하여 균일하게 분포된 해시값을 생성한
  - 해시된 결과를 기준으로 청크(Chunk)를 나누고, 각 청크는 여러 샤드에 고르게 분배
  - 해시 함수는 데이터가 특정 패턴으로 몰리지 않도록 균등 분포를 보장하여 부하 편중 문제를 줄인
  - 특징
    - 데이터가 특정 범위에 집중되는 문제(데이터 편향)를 완화해 모든 샤드에 데이터가 균등하게 분산된다.
    - 범위 쿼리(range query)나 연속된 값 조회에는 비효율적이며, 샤드 간 연속 범위 데이터 처리시 부적합하다.
    - 주로 분산과 부하 균형이 중요한 경우에 적합하다
  - 장점
    - 데이터가 균등 분산, 부하 편중 완화
  - 단점
    - 범위 쿼리에 적합하지 않음, 정렬이나 범위 기반 처리 어려움
- 샤드 키
  - 샤드 키와 데이터를 어떤 방식으로 분산할 것인지 결정하는 샤딩 알고리즘
    - 한번 설정하면 컬렉션을 완전히 새로 생성하지 않는 이상 변경할 수 없다.
  - 타겟 쿼리와 브로드캐스트 쿼리 결정 (쿼리 처리성능과 응답 시간)
    - 적은 데이터를 빠르게 읽어가는 가벼운 쿼리가 모든 샤드로 전송되면 그만큼 느려지고 샤드 서버는 부하를 받게 된다
    - 하지만 배치 형태의 프로그램이라면 모든 샤드를 골고루 활용할 수 있도록 유도하는 것이 좋다
  - 각 서드 서버의 부하 분산
    - 각 샤드 서버는 균등하게 데이터를 분산해서 갖게 된다
    - 각 청크의 샤드 키 범위도 중요하다
  - 청크 밸런스 작업
    - 샤드키는 쿼리가 균등하게 모든 샤드를 사용할 수 있도록 설계해야할 뿐아니라 저장되는 데이터도 균등하게 배포될 수 있게 고려

#### 프라이머리 샤드
- 프라이머리 샤드는 샤드 클러스터에서 샤딩되지 않은 컬렉션들을 저장하는 샤드
- 처음 DB가 생성되면 MongoDB 서버는 각 샤드 중 데이터를 가장 적게 가진 샤드를 선택해 생성되는 DB를 프라이머리 샤드로 설정
- 프라이머리 샤드는 다른 샤드로 옮겨질 수 있는 데, 제거하고자 하는 샤드가 프라이머리 샤드에 경우 다른 샤드로 옮겨야 한다.
  - db.runCommand( {movePrimary: "mysns", to: "shard-04"} )

#### 청크 밸런싱
- 샤드 클러스터 밸런서 (Balancer)
  - 샤드별 데이터 양을 모니터링, 특정 샤드에 편중될 경우 자동으로 청크를 이동시켜 균등 분배 유지 백그라운드 프로세스
  - 역할과 기능
    - 클러스터 내 각 샤드의 데이터 양 불균형을 감지하고, 마이그레이션 임계치에 도달하면 데이터 마이그레이션을 시작
    - 청크가 많은 샤드에서 적은 샤드로 청크를 이전하는 방법으로 샤드 간 데이터 분포를 균등하게 만든다.
    - 청크 크기와 밸런싱 과정의 오버헤드가 있으나, 최대 동시에 한 샤드당 1개의 마이그레이션만 수행하여 성능 영향 최소화
    - 밸런서 프로세스는 기본적으로 활성화되어 있으며, 필요시 비활성화하거나 실행 시간을 스케줄링할 수 있다.
    - 샤드 추가나 제거 시 데이터 불균형이 심해지면 밸런서가 자동 개입해 데이터 재분배
  - 특징과 영향
    - 밸런서는 클러스터 성능과 안정성을 높이고, 확장성과 부하 분산을 지원
    - 밸런싱 작업은 투명하게 수행되지만, 데이터 마이그레이션 시점에 네트워크와 디스크 I/O 부담이 발생 가능
    - 청크 메타데이터 업데이트는 라우팅 테이블 크기에 비례하며, 테이블이 크면 작업 중 CRUD 작업이 일시적으로 지연 가능
    - 일정한 청크 크기 유지와 균등한 청크 분포가 장기 성능 유지에 중요
  - 단점
    - 성능 영향 가능성
      - 청크 마이그레이션 과정에서 네트워크와 디스크 I/O 부하 증가
      - 샤드 서버와 클러스터 전체 성능 저하를 일으킬 수 있다. 특히 대용량 데이터 이동 시 영향이 크다.
    - 작업 중 지연 발생
      - 밸런서가 청크를 이동하는 동안 일부 CRUD 작업이 일시적 지연 가능
      - 라우팅 테이블 업데이트가 많을 경우에도 부하 증가 가능
    - 복잡한 운영과 관리
      - 밸런서 동작을 모니터링하고 필요에 따라 중지, 재시작, 스케줄링하는 등의 관리 작업이 필요해 운영 부담이 있다.
    - 비용 증가 요인
      - 샤드간 청크 이동은 추가 네트워크 사용과 서버 리소스 소비를 초래해 클라우드 환경 등에서 비용 상승 요인이 될 수 있다.
    - 이상 상황 처리 어려움
      - 밸런싱 중 네트워크 장애나 서버 문제 시 청크 이동이 중단되거나 불완전하게 완료되어 데이터 불균형이나 장애 가능
- 청크 스플릿
  - 하나의 청크가 너무 커져서 지정된 임계치를 초과할 때, 그 청크를 두 개의 작은 청크로 나누는 작업
  - 동작 원리
    - 데이터가 특정 청크에 몰려서 크기가 커지면, mongos나 샤드가 자동으로 이 청크를 분할한다.
    - 분할은 지정된 샤드 키 값을 기준으로 두 개의 하위 청크로 나누며, 각 청크는 다시 독립적인 범위와 데이터 집합을 가진다.
    - 자동 분할 외에도 관리자가 수동으로 특정 위치에서 split 명령을 내려 청크를 나눌 수 있다.
  - 목적과 효과
    - 청크 스플릿은 데이터 분산을 더 잘하고, 특정 샤드에 데이터가 과도하게 집중되는 것을 방지하여 부하 편중 문제 완화
    - 스플릿이 이루어진 후 밸런서가 자동으로 청크를 균등하게 샤드 간 분배하도록 마이그레이션 작업 수행
    - 적절한 청크 크기 유지로 샤딩 클러스터 성능과 확장성을 높이는 핵심 요소
- 청크 머지
  - 청크 머지(Chunk Merge)는 동일한 샤드에 연속된 다수의 작은 청크들을 하나의 큰 청크로 결합하는 작업을 말한다.
  - 청크 머지 개념과 원리
    - 샤딩된 컬렉션에서 데이터가 삭제되거나 분산 과정에서 청크가 너무 작아져 불필요하게 세분화된 경우
    - 청크 머지를 통해 여러 청크를 합쳐 관리 효율성과 성능을 높인다.
    - 머지는 기본적으로 동일한 샤드 내에서 인접한 청크들이 합쳐질 수 있는 조건을 만족할 때 수행되
    - 여러 개의 작은 청크들이 하나의 적정 크기 청크로 통합
    - 자동 머지 기능이 MongoDB에 내장되어 있지만 필요에 따라 수동으로도 명령을 실행할 수 있다.
  - 청크 머지 목적과 효과
    - 너무 작은 청크가 과도하게 많으면 청크 관리 오버헤드가 커지고, 쿼리 라우팅 비용과 밸런서 작업이 비효율 가능성
    - 청크 머지를 통해 청크 수를 적절히 줄이면 라우팅 테이블 크기를 줄이고, 성능을 최적화한다.
    - 메타데이터 업데이트와 라우터의 청크 캐시 동기화가 빨라져 클러스터 안정성 향상에도 도움이 된다.
- 청크 이동
  - 청크를 보내는 샤드에서의 7단계
    - moveRange 명령 수신: 밸런서가 소스 샤드에 moveRange 명령을 보내 청크 이동 시작을 알림.
    - 쓰기 작업 리디렉션: 소스 샤드는 해당 청크 범위에 들어오는 쓰기 작업을 수신 처리하며, 이동 중임을 기록.
    - 읽기 작업 허용 유지: 청크에 대한 읽기 작업은 계속 허용됨.
    - 데이터 복사 시작: 소스 샤드가 대상 샤드에게 데이터 복사 요청을 보냄.
    - 동기화: 대상 샤드가 초기 데이터 복사 후 변경사항을 동기화하며 일관성 보장.
    - 메타데이터 업데이트: 복사가 완료되면 클러스터 메타데이터에 청크가 대상 샤드로 이동되었음을 반영.
    - 원본 청크 데이터 삭제: 소스 샤드는 이동 완료 후 자신에게 남은 청크 데이터를 삭제.
  - 청크를 받는 샤드에서의 8단계
    - 데이터 복사 준비: 대상 샤드는 필요한 인덱스 구축 시작.
    - 데이터 수신 및 저장: 소스 샤드에서 데이터 청크를 수신하여 저장 및 인덱싱 수행.
    - 변경사항 동기화: 마이그레이션 시 발생한 쓰기 작업을 동기화해 데이터 일관성을 유지.
    - 완료 신호 전송: 동기화 및 복사가 완료되면 소스 샤드에 완료 신호 전달.
    - 메타데이터 갱신 대기: 클러스터 관리자가 메타데이터를 갱신하는 동안 대기.
    - 쿼리 처리 정상화: 청크를 대상으로 수행되는 읽기/쓰기 요청을 정상 처리 시작.
    - 오래된 청크 애플리케이션 제거: 이동한 구간에서의 읽기 요청이 없어진 것을 확인하며 삭제 준비.
    - 완전한 마운트 완료: 최종적으로 청크가 새 샤드에 완전히 마운트되고 완전한 운영 상태에 진입.
  - 이 과정 동안 데이터는 일관성을 유지하면서 최소한의 서비스 중단으로 샤드 간에 이동한다.
  - 각 단계는 청크 이동 안정성과 클러스터 성능에 중요한 영향을 미친다.
  - MongoDB 샤드 클러스터에서 청크 이동 과정은 소스 샤드(보내는 쪽)와 대상 샤드(받는 쪽)에서 각각 단계별로 진행된다
- 청크 사이즈 변경
  - 청크 사이즈의 기본값은 64MB 이다.
  - 샤드 클러스터에서 부하 분사의 가장 기본이 되는 옵션이며, 유연한 청크 이동, 샤드 간 청크 이동 시 발생하는 부하 조절, 샤드 간 부하 분산의 정도를 결정하는 요소
  - 청크의 크기는 하나의 청크가 가지는 도큐먼트의 개수 결정
    - 필요에 따라 청크 사이즈 변경: db.settings.save( {_id: "chunksize", value: 100}) (100MB 변경)
  - 청크 사이즈 변경할 때 자동 청크 스플릿 처리 과정
    - 해당 청크에 INSERT, UPDATE가 실행될 때만 자동 청크 스플릿 실행
    - 청크 사이즈를 줄이는 경우 즉시 청크가 스플릿되는 것이 아니라, 각 청크에 INSERT, UPDATE가 실행될 때 새로운 청크 사이즈를 기준으로 스플릿
    - 청크 사이즈가 늘리는 경우 각 청크는 새롭게 설정된 청크 사이즈만큼 커질 때까지 스플릿되지 않는다

#### 샤딩으로 인한 제약
- 트랜잭션
  - MongoDB에서 단일 도큐먼트(단일 문장X)에 대한 변경은 트랜잭션을 지원한다. (WiredTiger 스토리지 엔진)
    - 롤백을 할 수 있는 방법은 없지만, 성공/실패로 결정된다는 것
    - 단일 도큐먼트 변경은 원자성을 가지고 처리된다. 하지만 여로 도큐먼트를 변경하는 작업은 원자성을 가지지 않는 다는 것
  - MongoDB를 단일 인스턴스로 사용해도 RDBMS 처럼 트랜잭션을 사용할 순 없다.
    - RDBMS를 샤딩으로 구성하여도 트랜잭션을 사용할 수 없다
- 샤딩과 유니크 인덱스
  - MongoDB의 프라이머리키는 샤딩과 무관하게 항상 유니크해야 한다.
    - 세컨드리 키 중에서도 유니크 옵션이 설정되는 경우 프라이머리 키와 동일하게 중복을 허용하지 않도록 처리해야 한다.
  - 기본 원리
    - MongoDB의 _id 필드는 각 컬렉션에서 반드시 유일해야 하며, 프라이머리 키로 동작
      - 샤딩 환경에서도 기본적으로 _id 필드는 컬렉션 전체, 즉 전체 샤드에 대해 고유성을 보장한다.
      - ObjectId 기반의 _id는 전역적으로 충돌 확률이 극히 낮아 자동으로 고유성을 띤다.
    - 임의의 값이나 다른 필드로 유니크 인덱스를 설정한 경우 샤드 아키텍처와 샤드 키에 따라 고유성 적용 범위가 달라질 수 있다.
  - 샤드 클러스터에서 중복 방지 방식
    - _id를 샤드 키로 사용하면: 동일한 _id 값이 들어오는 모든 요청이 같은 샤드의 같은 청크로 라우팅된다.
      - 해당 샤드에서만 중복 체크가 진행되고 샤딩 클러스터 전체에서 고유성이 안전하게 유지된다.
    - _id 이외의 필드에 유니크 인덱스 사용 시: 해당 필드를 샤드 키 또는 샤드 키의 프리픽스(prefix)로 지정하면 유니크 인덱스 제약이 전체 클러스터에 걸쳐 동작
      - 샤드 키와 상관없는 필드에 유니크 인덱스를 설정하면 각 샤드 내에서만 적용될 수 있어 전역 유니크 보장이 되지 않는다.
      - 반드시 샤드 키와 유니크 인덱스 구성에 주의가 필요하다.
  - 샤드 키와 유니크 인덱스가 다를 때: 대량의 동시 insert 등이 발생하면 서로 다른 샤드에서 중복값이 들어갈 위험이 있으므로, 이 경우에는 어플리케이션 레벨에서 중복 체크 보완이 필요하다.
  - 복합 유니크 인덱스(ex. {platform_id: 1, user_id: 1})로 샤딩 설정시
    - 두 필드의 조합에 대해서만 전역 고유성 보장이 가능하다.
    - 만약 필드 중 하나(user_id)에 대해서만 전역 고유성이 필요하면, DB 레벨 보장이 불가능하고 어플리케이션에서 체크해야 한다.
- 조인 한계
  - 복잡한 조인 불가: 샤드 클러스터는 RDBMS처럼 여러 테이블(컬렉션) 간 복잡한 조인 연산을 지원하지 않는다.
    - 일부 조인 기능을 위한 $lookup 오퍼레이션이 있지만, 샤드된 컬렉션을 대상으로는 지원에 제한이 있다.
    - $lookup에서 "from"에 지정한 컬렉션이 샤딩된 경우, 샤드 간 분산 데이터 병합(workload)이 어려워 성능 저하 및 오류 위험이 존재
  - 쿼리 성능↓/비효율: 샤드 키를 포함하지 않은 조인 쿼리는 브로드캐스트 후 병합해야 하며, 네트워크와 I/O 오버헤드가 매우 커진다.
  - 설계상 권장되지 않음: 샤딩은 문서 단위 저장 및 조회에 최적화돼 있어, 복잡한 조인 쿼리가 필요한 경우 샤딩 구조와 맞지 않는다.
- 그래프 쿼리 한계
  - $graphLookup 제약: 계층적(재귀적) 그래프 쿼리를 위한 $graphLookup 역시, 대상 컬렉션이 샤딩된 상태라면 사용할 수 없다.
    - 모든 계층형, 그래프 기반 탐색 쿼리는 샤드 클러스터 환경에서는 지원이 안 되거나 심각하게 성능이 저하된다.
  - 샤드 간 참조 불가: 다른 샤드의 데이터를 직접 참조할 수 없어 분석용/그래프 탐색/계층조직 조회 등 원활히 수행할 수 없다.
- 기존 컬렉션에 샤딩
  - 샤딩되지 않은 상태로 데이터를 가지고 있는 컬렉션을 샤딩할 때에는 샤딩을 적용할 수 있는 컬렉션의 사이즈 제한이 있다
  - 우선 컬렉션에 대해 청크 스플릿 실행
    - splitVector 명령 실행: 하나의 BSON 도큐먼트를 반환하며 16MB 를 넘을 수 없다
    - 해당 제약사항으로 인해 256GB 이하의 컬렉션은 샤딩 적용 지원 가능하다고 메뉴얼 언급
  - 큰 컬렉션에 대해 샤딩을 적용하기 위해서는 청크 사이즈를 일시적으로 2, 4배로 설정한 다음 컬렉션을 샤딩해야 한다
