### 01. MongoDB

#### MongoDB vs MySQL
- 객체의 이름이 조금 다를 뿐 RDBMS와 비슷한 역할을 한다.
  - 데이터베이스, 컬렉션, 도큐먼트, 필드, 인덱스
- 쿼리 결과를 'Cursor' 로 반환하는 데, 커서를 통해 반복적으로 실제 도큐먼트(레코드)를 가져올 수 있다.
- MongoDB 특징
  - NoSQL
    - 기본적으로 SQL을 사용하지 않지만 MongoDB Connector for BI, Simba의 SQL Driver 등을 사용하면 비슷하게 통신 할 수 있다.
    - 외래키를 명시적으로 지원하지 않지만 $lookup 이라는 Aggregation 기능을 이용하면 조인 처리를 수행(샤딩 제약)할 수 있다.
    - SQL, NoSQL 간의 경계가 허물어지고 있다.
  - Schema-Free
    - 가장 큰 차이점으로 사용할 컬럼을 미리 정의하지 않고 언제든 필요한 시점에 데이터를 저장할 수 있다.
  - 비 관계형 데이터베이스
    - SQL 문법을 지원하지 않고 자바스크립트 기반의 명령을 이용하는 데, JSON 도큐먼트를 인자로 사용

#### MongoDB 배포 형태
- MongoDB 도 HBase, Casandra 같이 클러스터 형태로 서비스할 수 있도록 구현된 DB 서버다.
  - 하지만 꼭 클러스터 형태로 구성해야만 사용할 수 있는 것은 아니다.
- 배포형태는 MySQL 서버의 구조와 매우 비슷하다.
  - 단일 서버로도 사용 가능하며 복제 또는 샤딩된 구조로도 활용 가능하다.
- 단일 노드
  - 해당 배포 형태는 MongoDB는 복제를 위한 로그(OpLog)를 별도로 기록하지 않으며 다른 노드와 통신도 필요하지 않다.
  - 자동 페일오버나 HA 기능이 작동할 수 없어 주로 개발 서버의 구성에 사용
- 단일 레플리카 셋
  - 레플리카 셋 구축을 위해 추가로 MongoDB 서버 필요
  - 장애 상황에서 자동 복구를 위한 최소 단위로 바동 복구가 필요하다면 레플리카 셋으로 MongoDB 배포 필요
  - 노드 간 투표를 통해 Primary 노드를 결정하므로 가능하면 홀수 개의 노드로 구성하는 것이 좋다
  - Arbiter 모드
    - replica set 구성에서 장애 조치와 리더 선출을 위해 투표만 하는 역할을 수행하는 노드 모드입니다.
    - Arbiter의 역할과 특징
      - Arbiter는 데이터 복제를 하지 않고, 오로지 투표 목적으로만 존재합니다.
      - Primary와 Secondary 노드에서 선거(election)가 발생할 때 Arbiter가 투표에 참여해 quorum(과반수)를 채워 줍니다.
      - Arbiter는 직접적인 데이터 저장이나 읽기, 쓰기 연산을 하지 않아 서버 리소스를 거의 소모하지 않습니다.
      - 장애 whcl(failover) 시, Primary 장애 발생 시 Secondary 승격 여부를 결정합니다.
- 샤딩된 클러스터
  - 샤딩된 클러스터에 참여하고 있는 각각의 레플리카 셋을 샤드라고 하는 데, 이 샤드들이 어떤 데이터를 가지는지에 대한 정보는 MongoDB Config 서버 필요
  - 샤딩된 클러스터에서 MongoDB 드라이버는 MongoDB 라우터(mongos)로 연결하고, 자동으로 컨피그 서버로부터 각 샤드가 가지고 있는 데이터에 대한 메타 정보들을 참조하여 쿼리 실행
  - 요청된 쿼리를 실제 데이터를 가지고 있는 샤드로 전달하는 역할 수행, 이후 결과를 정렬 및 병합해 반환

### 02. Storage Engine
<img width="561" height="283" alt="image" src="https://github.com/user-attachments/assets/d8a501a7-6efe-4f51-a50d-29e15b9fed2e" />
  - 출처: https://hoing.io/archives/4561


#### 플러그인 스토리지 엔진
- 스토리지 엔진들은 사용자의 데이터를 디스크에 영구적으로 기록하거나 다시 읽어와서 메모리에 적재하는 역할 담당
  - MongoDB는 MMAPv1, WiredTiger 스토리지 엔진을 선택하여 사용할 수 있고 동시에 사용할 수 없다.
- 옵티마이저라고 부르는 컴포넌트가 처리를 담당하는 데, 최적화된 실행 계획을 수립해준다.

#### MMAPv1 스토리지 엔진
- MMAPv1 특징
  - MongoDB 3.0 까지 주로 사용되던 스토리지 엔진으로 2.6까지는 데이터베이스 단위의 잠금 사용
    - DML 문장의 동시 처리 성능이 좋지 않았는데, 컬렉션 수준 잠금으로 개선되면서 동시성 처리로 변경, 하지만 동시성 처리에 많은 걸림돌이 된다.
  - 내장된 캐시 기능이 없어 운영체제의 캐시 활용
    - 윈도우 캐시를 사용하기 때문에 System call 을 거치게 되어 오버헤드가 상대적으로 크다
- 데이터 파일 구조
  - 데이터 파일은 데이터베이스 단위로 생성된다.
    - storage.smallFiles 옵션을 이용하면 크기를 작게 만들 수 있다. (default 64MB)
  - stage.directoryPerDB 옵션은 디스크의 데이터 파일을 데이터베이스 단위로 별도의 디렉터리에 저장할 것인지 결정
    - true / false
  - database.ns 파일
    - 몽고DB 서버에 생성된 데이터베이스와 컬렉션 그리고 인덱스 정보 저장
  - systemLog.path, processManagement.pidFilePath 옵션을 이용해 생성될 디렉터리

- MongoDB 서버 상태 확인
  - mongostat 도구를 통해 현재 서버의 쿼리 처리량, 메모리 사용량과 같은 전체적인 상태를 볼 수 있다

- 운영체제 캐시
  - 내장된 캐시가 없고 OS에서 제공하는 패이지 캐시 기능에 의존
    - 내장된 캐시가 없기 때문에 더티 페이지를 효율적으로 관리하는 기능이 없다.
    - 많이 발생하는 문제는 Cache Invalidation 현생 (페이지 캐시 삭제)
  - 리눅스 페이지 캐시에 의존하기 때문에 커널의 작동 방식, 파라미터 튜닝이 필수적이며 그 중 디스크의 페이지를 읽어들이는 방식과 페이지 캐시가 갖고 있는 더티 페이지가 디스크를 동기화 하는 방식에 대한 이해가 필수적이다.
  
- 데이터 파일 프레그멘테이션
  - Fragmentation: 도큐먼트의 크기가 계속 증가하면 기존의 작은 공간들은 재활용하지 못하고 계속 빈 공간이 남게 되는 데, 이런 현상이 반복되면 사용하지 못하는 공간이 늘어나게 된다.
  - Fragementation 가 많으면 같은 블록 하나를 읽어도 가져올 수 있는 도큐먼트 개수가 달라지므로 메모리 효율성, 성능 차이가 발생
  - db.collection.stats() 명령어를 통해 확인 가능
    - count: 도큐먼트 건수
    - size: 컬렉션의 전체 도큐먼트 크기
    - storageSize: 컬렉션을 위해 할당된 전체 디스크 데이터 파일 크기
    - avgObjSize: 도큐먼트 하나의 평균 크기
    - paddingFactor: MMAPv1 스토리지 엔진을 사용하여 컬렉션에서만 패딩 사용
  - 디스크 데이터 파일 크기를 줄이기 위해서 컴팩션하거나 세컨드리 멤버의 데이터를 덤프한 후에 다시 적재
    - compaction을 수행하는 경우에 paddingBytes, paddingFactor 옵션 추가 가능

#### WiredTiger 스토리지 엔진
- 특징
  - 내부적인 잠금 경합 최소화(Lock-free algorithm)을 위해 "하자드 포인터", "스킵 리스트" 와 같은 기술 채택
  - MVCC와 데이터 파일 압축, 암호화 기능들을 모두 가지고 있다
- 저장 방식
  - 3가지 타입의 저장소
    - 레코드 스토어: 컬렉션의 레코드를 한번에 저장하는 방식으로 B-Tree 알고리즘 사용
    - 컬럼 스토어: 대용량의 분석 용도로 많이 사용되며 컬럼 단위 또는 컬럼 그룹 단위로 데이터 파일 관리, 데이터 파일 크기가 작아 읽어들이는 속도가 매우 빠르다
    - LSM(Log Structured Merge Tree) 스토어
      - 읽기보단 쓰기에 집중된 저장방식으로 순차 파일 형태로 데이터를 저장하며 시간 순서대로 1개 이상이 관리된다.
      - 메모리에 디스크로 갓 저장된 파일을 Level-0 파일이 되고 조각이 많아지면 이들을 모아 Level-1 데이터 파일 조각을 만든다.
      - 이렇게 n까지 계속 성장하는 방식으로 작동하며 레벨이 높을수록 오래된 데이터로 구성
      - 읽을 떄는 N개의 데이터 파일을 읽어야 하기 때문에 읽기 성능은 떨어지나 대용량의 insert를 문제없이 처리하기 위해 사용
- 데이터 파일 구조
  - MMAPv1 스토리지 엔진과는 다른 디렉터리와 데이터 파일 구조 사용
  - storage.directoryPerDB 옵션을 이용해 데이터베이스별 디렉터리를 구분할 수 있다.
    - 파일의 목록이나 내용에는 차이가 없으며 단지 컬렉션의 데이터 파일이 생성되는 위치가 전용 디렉터리를 사용하는지 아닌지 차이
  - WiredTiger: 텍스트 파일로 현재 실행 중인 WiredTiger 스토리지 엔진 버전 저장
  - storabe.bson: BSON 포맷으로 설정되며 storage.directoryPerDB, storage.directoryForIndexes 옵션 값 저장
  - sizeStorer.wt: 전체 도큐먼트 건수와 각 컬렉션의 데이터 파일 크기 저장
    - 샤딩된 클러스터 환경, 샤드간 데이터 재분산 중, MongoDB 비정상 종료, 고아가 된 다큐먼트가 있을 땐 컬렉션의 도큐먼트 건수를 보여주지 않을 수 있다.
  - WiredTiger.lock: 다른 몽고 서버 인스턴스가 동시에 사용하지 못하도록 잠금 역할하기 위한 파일
    - 재시작되면서 엔진을 초기화할 때 WiredTiger.lock 파일이 남아 있으면 서버가 비정상적으로 종료됐다고 판단, 복구 모드 시작
  - WiredTiger.turtle: 엔진의 설정 내용을 담고 있다
  - WiredTiger.wt: 엔진의 메타 데이터를 저장하는 컬렉션의 데이터 파일
  - _mdb_catalog.wt: 스토리지 엔진 컬렉션과 인덱스의 목록, 메타 데이터를 관리하는 파이
  - WiredTigerLAS.wt: 데이터 페이지들이 더티 페이지 상태여서 디스크에 기록을 해야할 때 해당 파일 사용
    - 캐시에서 재사용할 수 있는 공간이 부족하면 eviction server thread 는 필요한 만큼의 여유 공간을 만들어야 사용자 요청을 원할히 처리할 수 있다.
  - diagnostic.data: 수집된 내부 정보를 1초에 한번씩 모아 별도의 파일로 기록 
- 내부 작동 방식
  - <img width="694" height="529" alt="image" src="https://github.com/user-attachments/assets/123e6c4c-a678-4eb2-b089-6f8ab33f982e" />
    - 출처: https://hoing.io/archives/4561
  - 트랜잭션을 지원하는 RDBMS와 유사한 내부구조로 되어 있다.
  - 다른 DBMS와 동일하게 B-Tree 구조의 데이터 파일과 서버 크래시로부터 데이터 복구하기 위한 저널 로그(WAL)를 가지고 있다.
  - WiredTiger.configString 옵션에 저널 로그와 관련된 설정 변수
    - enabled(저널로그 활성화 여부), archive(체크포인트 이전의 저널로그는 자동 삭제하는 데, 아카이빙하여 보관할 지 여부)
    - file_max(저널 로그 파일의 최대 크기 설정), path(새로운 저널 로그의 디렉터리 경로 설정)
  - 스토리지 엔진 내장된 공유 캐시(버퍼 풀)을 가지고 있다.
    - 쿼리 처리 속도 개선, 쓰기 배치 기능 가능
    - 블록 매니저를 통해 필요한 데이터 블록을 읽어 공유 캐시에 적재하여 쿼리를 처리
- 공유 캐시
- 캐시 이빅션(Cache Eviction)
- 체크 포인트
- MVCC (Multi Version Concurrency Control)
- 데이터 블록 (페이지)
- 운영체제 캐시 (페이지 캐시)
- 압축
- 암호화

### 03. 복제
- 마스터-슬레이브 복제는 초기 사용했던 복제 방식으로 현재는 레플리카 셋 복제로 많은 부분 자동화되었다

#### 복제란(Replication)?
- 여러 서버가 서로의 데이터 동기화를 의미하며 물리적, 논리적 복제로 나눌 수 있다.
  - DRBD(Distributed REplicated Block Device)와 같이 리눅스 서버가 데이터의 내부를 전혀 모르는 상태에서 디스크의 블록만 복제하는 형태가 물리적 방식
  - MySQL, MongoDB 같이 서버간의 데이터를 동기화 하는 방식을 논리적 방식
- Raft 컨세서스 알고리즘
  - 가장 큰 특징은 리더 기반의 복제와 각 멤버 노드가 상태를 가진다는 것
    - 하나의 레플리카 셋에는 반드시 하나의 리더만 존재할 수 있고, 리더는 사용자의 모든 데이터 변경 요청 처리
  - 리더는 요청 내용을 로그에 기록하고 모든 팔로워는 리더의 로그를 가져와 동기화 수행
    - 해당 로그를 OpLog(Operation Log)라 한다
- 복제 목적
  - HA(High Availability: 고가용성)
    - 복제의 가장 큰 목적은 동일한 데이터를 2, 3중 유지하므로써 데이터 손실이 발생해도 데이터를 대체할 수 있게하기 위함
  - 데이터 조회 쿼리 로드 분산
    - 보통 고가용성을 위해 3개의 레플리카로 구성하는 데 조회 쿼리를 분산하기 위해 멤버를 추가할 수 있다.
  - MongoDB는 물리적인 백업 기능을 제공하고 있지 않다.
    - 때문에 긴급하게 데이터 북구해야하는 시점에 secondary 멤버를 멈추고 데이터 파일을 복사해야할 수 있다.
   
#### 레플리카 셋 멤버
- 프라이머리
  - 데이터 변경을 처리할 수 있는 유일한 멤버로 사용자의 변경 요청을 처리하고, 변경된 데이터를 OpLog에 기록하여 다른 멤버들이 동기화
  - 프라이머리가 응답 불가능 상태가 되면 응답을 처리할 수 없게 된다.
    - 세컨더리 멤버들이 하트비트 메세지로 체크하고 있기 때문에 즉시 알아차리고 새로운 프라이머리 선출 투표 진행한다
- 세컨더리
  - 프라이머리로부터 데이터를 실시간으로 가져와 동기화를 하여 동일한 데이터 셋 유지
  - 데이터 변경 요청을 직접 처리할 수 있지만 읽기 요청을 직접 처리하여 부하 분산 용도 가능
  - 데이터 유실 가능성이 있기 때문에 백업 용도로는 어렵다.
- 아비터
  - 아비터는 프라이머리 선출 투표에서 정족수를 채우기 위해 사용

#### 프라이머리 선출
- 레플리카 셋에 현재 프라이머리 멤버가 없으면 프라이머리 선출 진행
  - 해당 레플리카 셋에 프라이마리 멤버가 없으면 사용자 변경 요청을 처리 할수 없다.
  - Read Preference 옵션에 따라서 읽기 쿼리 조차도 불가능할 수도 있다.
- 프라이머리 텀(Term)
  - Protocol Version 0
    - MongoDB 3.0 버전까지의 레플리카 셋은 프라이머리 텀(term) 이라는 개념이 없었다.
      - 프라이머리 선출 과정에서 여러 멤버가 동시에 투표를 하면 중복 투표 가능
    - MongoDB 3.2 이전 버전에서는 프라이머리 선출 투표 를 30초에 한 번만 실행될 수 있도록 설계
      - 프라이머리 선출을 위한 투표가 한번 실패하게 되면 그 레플리카 셋은 30초 동안 프라이머리가 없는 상태로 대기
    - 2 단계 투표(Two-Phase Election)
      - 이런 중복 투표나 30초 대기 시간이 최대한 발생하지 프라이머리 선출 시 사전 투표(Speculative-election)와 본 투표(Authoritative-election) 나누어 진행
      - 사전 선거를 통해서 본 선거의 실패 상황(프라이머리를 선출하지 못하는 상황)을 최소화하고자 한 것
  - Protocol Version 1
    - 복잡한 선출 과정을 해결하기 위해서 프라이머리 텀(논리적인 시간의 의미로 사용)이라는 개념 도입
    - Perimary Term이란 투표 식별자이며, 레플리카 셋의 각 멤버들이 프라이머리 선출을 시도할 때마다 1씩 증가하는 논리적 시간
    - 각 멤버들은 투표 요청이 식별자를 기준으로 자기가 투표를 했는지 아니면 다시 투표에 참여하는지 결정 할 수 있다.
    - 단순히 투표를 할 때만 사용되는 것이 아니라 프라이머리 멤버가 사용자의 데이터 변경 요청을 실행한 다음 변경 내용을 OpLog 에 기록할 때마다 현재 텀(Term) 식별자를 같이 기록하여 특정 OpLog가 어느 멤버가 프라이머 였을 때 로그 인지를 식별 가능하게 한다.
   
- 프라이머리 스텝 다운
  - 레플리카 셋 설정된 electionTimeoutMillis 내에 응답이 없다면 레플리카 셋의 각 멤버는 현재 Primary가 없어졌다고 판단
    - 새로운 Primary 선출을 위한 투표를 시작하게 됩니다.
  - 관리자가 의도적으로 프라이머리를 세컨더리로 내리는(Step Down) 작업도 할 수 있다.
    - rs.stepDown() 명령으로 프라이머리를 스텝 다운함
    - rs.reconfig() 명령으로 레클리카 셋 멤버의 우선 순위(Priority) 를 변경함
  - rs.stepDown(stepDownSecs, secondaryCatchUpPeriodSecs)
    - 이 명령이 실행되면 즉시 프라이머리를 내려놓고 stepDownSecs 시간동안 다시 프라미머리가 될 수 없다
    - 이 기간동안 프라이머리가 선출되지 못한다면 해당 프라이머리가 다시 프라이머리가 될 수 있다.
    - secondaryCatchUpPeriodSecs 시간동안 새로운 프라이머리를 선출하지 않고 기다리면서 밀려 있던 복제가 모두 완료(동기화) 되기를 기다린다.
  - rs.config()
    - 명령은 사실 세컨드리를 프라이머리로 전환하는 등의 레플리케이션의 역할(Role) 을 변경하는 직접적인 명령어는 아니다.
    - 하지만 rs.reconfig() 명령으로 레프리카 셋 멤버의 priority 를 변경하여 프라이머리를 세컨더리로 변경할 수 있다.
- 프라이머리 선출 시나리오
  - 프라이머리 선출 트리거 상황
    - 기존 프라이머리 노드가 하드웨어 장애, 네트워크 단절 등으로 인해 더 이상 정상 동작하지 않을 때 선출 발생
    - 프라이머리가 현재 셋에 존재하지 않는다면, 세컨더리 노드가 이를 감지하여 즉시 투표 프로세스를 개시한다.
  - 선출 방식과 과정
    - MongoDB는 Self-Election(자기 선출) 방식을 채택하고 있다.
    - 각 세컨더리 노드는 프라이머리가 필요하다는 사실을 인지하면 자기 자신을 후보로 하여 투표 과정을 시작한다.
  - 투표 조건
    - 후보 노드가 같은 레플리카 셋 소속인지
    - 후보 노드의 우선순위(priority)가 가장 높은지
    - 요청한 선출 Term(term)이 현재 자신이 아는 값보다 최신인지
    - 기존에 해당 Term에 투표한 적이 없는지
    - 데이터 동기화 상태(OpTime)가 가장 최신 또는 동등한지
  - 투표 및 정족수
    - 과반수 이상의 투표를 받아야 프라이머리로 선출된다.
    - 멤버의 votes 값이 1이어야만 정족수에 포함된다. Non-Voting 멤버는 투표 인원 산정에서 제외된다.
    - 스플릿 브레인(Split-brain) 방지: 과반수 연결이 안 되면 기존 프라이머리도 세컨더리로 자동 강등된다.
  - 기타 고려 사항
    - 후보보다 더 우선순위가 높거나, 더 최신 데이터를 가진 멤버가 있으면 투표가 거부된다.
    - 프라이머리 선출 실패 시, 지체 없이 새로운 투표가 다시 시작될 수 있다(Protocol Version 1).
    - 복구 과정이나 동기화 상태(STARTUP 등)에 따라 투표에 참여하지 못하는 멤버도 생길 수 있다.
- 롤백 및 롤백 데이터 재처리
  - 롤백 발생 원인 및 과정
    - 프라이머리에서 작성된 데이터가 세컨더리로 복제되기 전에 프라이머리 장애 또는 선출(Primary election)이 발생하는 경우
    - 해당 시점 이후 프라이머리에 기록된 데이터가 세컨더리에는 적용되지 않는다.
    - 장애 후, 해당 노드(이전 프라이머리)가 세컨더리로 다시 합류하면, 자신만 가지고 있는(다른 노드와 공유되지 않은) OpLog를 기준으로 롤백 과정을 진행한다.
    - 이 과정에서 새로운 프라이머리의 OpLog를 기준으로 자신과 일치하지 않는 트랜잭션 및 쓰기를 삭제(되돌리기)한다.
    - 즉, Primary만 갖는 데이터를 동기화하는 것이 아닌 삭제하는 작업
  - 롤백의 내부 동작
    - 이전 프라이머리였던 멤버는 자신의 OpLog를 역순으로 검사하여 공통되는 지점을 찾고, 그 이후의 OpLog를 삭제
    - 롤백 과정에서 OpLog만 삭제하는 것이 아니라, 관련 실제 데이터도 컬렉션에서 제거 또는 원래 상태로 복원
    - 롤백으로 인해 삭제‧변경된 데이터들은 서버의 데이터 디렉터리 하위 rollback 폴더에 BSON 파일로 저장된다.
    - 필요 시 bsondump 유틸리티로 변환하여 수동 복구가 가능
  - 롤백 한계 및 수동 재처리
    - 롤백 데이터의 크기가 300MB를 넘으면 자동 롤백은 실패하며, 이 경우 에러 로그가 남고 수동 복구가 필요하다.
    - 수동 재처리 시 rollback 디렉터리의 데이터를 관리자 또는 개발자가 직접 검토 및 복원해야 한다.
      - bsondump 유틸리티로 해당 BSON 파일의 내용을 확인하거나 JSON 형식으로 변환할 수 있다.
        - bsondump <파일명>.bson > <파일명>.json
      - 복구 대상 데이터가 확실하다면 mongorestore 명령어로 롤백된 BSON 파일을 원하는 데이터베이스 또는 컬렉션으로 복구
        - mongorestore --host <dbhost> --port 27017 --db <database> --collection <collectionName> <파일경로>/<파일명>.bson
      - 필요한 경우 --drop 옵션을 추가해 기존 데이터베이스나 컬렉션을 덮어쓸 수도 있다.
  - 트랜잭션 내의 롤백과 차이
    - 트랜잭션 롤백은 커밋되지 않은 변경 사항이 취소되는 메커니즘
    - 레플리카 셋의 롤백은 복제 불일치로 인한 데이터 정합성을 맞추기 위해 이미 기록된 데이터를 되돌린다는 점에서 다르다.

### 04. 샤딩
#### 샤딩이란
- 샤딩이란
  - 샤딩은 데이터를 여러 서버에 분산하여 저장하고 처리할 수 있도록 하여 고가용성과 대용량 분산 처리를 위해 복제와 샤딩 모두 사용
  - 샤드 클러스터를 구축해야 하는 데, 샤드 위치 정보 등 메타 정보를 저장하기 위한 컨피그 서버가 필요하며 mongos(프록시 역할) 라는 라우팅 서버 필요
- 샤딩의 필요성
  - 데이터베이스 서버를 확장하려면 데이터베이스의 데이터가 여러 서버로 분산될 수 있게 한다
  - 스케일 업에 경우 빠르게 한계에 이를 수 있다.
- 샤딩의 종류
  - 샤딩은 데이터를 어떤 형태로 파티션할지에 따라 수평, 수직 샤딩으로 구분하는 데, 수평 또는 수직 파티션이라고 한다
    - 수직 샤딩: 기능별로 컬렉션을 그룹핑하여 그룹벼로 샤드를 할당하는 방식으로 일반적으로 초기에 간단히 샤딩이 필요한 경우 사용
    - 수평 샤딩: 도큐먼트를 영역별로 파티셔닝하여 1/N개씩 각 샤드가 나눠 가지는 방식
    - 수평샤딩은 파티셔닝의 기준이 되는 필드(샤딩키) 선정이 매우 중요하며 이에 따라 각 샤드의 부하가 균등해질 수 있고 아닐 수 있다
  - 처음에는 샤드 클러스터 없이 서비스를 진행하다 사용량이 늘어나면 수직 샤딩 방식을 고려하여 컬렉션 단위로 묶어 나누어 구축
  - 이후 사용자가 더 늘어 데이터가 기하급수적으로 늘어나면 수평 샤딩을 조금씩 적용하게 되어 DB 구성이 진화된다
  - MySQL과 MongoDB
    - MySQL은 트랜잭션 관리가 필요할 때 활용하는 것이 좋다

#### 샤딩 아키텍처
- 샤드 클러스터를 구성하기 위해서 가장 중요한 3가지 컴포넌트는 샤드 서버, 컨피그 서버, 라우터(mongos)이다
- 샤드 클러스터 컴포넌트
  - 라우터는 영구적인 데이터를 가지지 않고, 쿼리 요청을 어떤 샤드로 전달할지 정하고, 쿼리 결과 데이터를 병합하여 리턴한다
  - 컨피그 서버는 영구적으로 데이터를 저장하는 데, 샤드 서버는 실제 사용자의 데이터를 저장하는 반면 컨피그 서버는 샤드 서버에 저장된 사용자 데이터가 어떻게 나누어 분산돼 있는지 메타 정보 저장
- 샤드 클러스의 쿼리 수행 절차
  - 컨피그 서버는 샤딩이 활성화된 DB와 컬렉션 정보를 관리
    - 샤드 클러스터에 컬렉션을 생성해도 샤딩이 되지 않은 객체들은 컨피그 서버가 아니라 각 샤드 서버가 로컬로 관리
  - 라우터의 쿼리 절차
    - 사용자 쿼리가 참조하는 컬렉션의 청크 메타 정보를 컨피그 서버로부터 가져와 라투어 메모리에 캐싱
    - 사용자 쿼리의 조건에 샤딩 키 조건을 찾음
      - 쿼리 조건에 샤딩 키가 있으면 청크 정보를 라우터의 캐시에서 검색하여 해당 샤드 서버로만 사용자 쿼리 요청
      - 샤딩 키 조건에 포함된 청크가 여러 샤드에 걸쳐있다면 대상이 되는 여러 샤드 서버에 쿼리 요청
      - 쿼리 조건에 샤딩 키가 없으면 모든 샤드 서버로 사용자의 쿼리 요청
    - 쿼리를 전송한 대상 샤드 서버로부터 쿼리 결과가 도착하면 결과를 병합하여 사용자에게 쿼리 결과 반환
- 컨피그 서버: 샤딩된 클러스터를 운영하는 데 있어 필요한 정보 저장
  - databases
    - _id(DB 이름 저장), partitioned(샤딩 활성화 여부), primary(프라이머리 샤드가 어느 샤드인 지)
  - collections
    - 샤드 클러스터가 가지고 있는 컬렉션 목록 관리
    - _id(db, collection 이름으로 구성), lastmod(마지막 변경 시점), dropped(삭제된 컬렉션 여부), key(샤딩 키), unique(샤딩키의 unique 여부), lastmodEpoch(컬렉션의 마지막 변경 일시를 ObjectId 포맷으로 저장)
  - chunks
    - 샤딩된 컬렉션의 모든 청크 정보 관리
    - _id(청크 아이디 저장), lastmod(마지막 변경 시점), lastmodEpoch(마지막 변경 일시를 ObjectId 포맷으로 저장), ns(네임스페이스), min(청크 시작 값), max(청크 종료 값), shard(저장된 샤드의 이름 저장)
  - shards
    - 샤드 크럴스터에 등록된 모든 샤드 서버의 정보를 레플리카 셋 단위로 관리
  - mongos
    - 라우터를 파악할 수 있도록 30초 단위로 샤드 클러스터의 모든 멤버와 ping을 주고 받는다
    - mongos에는 현재 컨피그 서버와 연결했던 모든 mongos 목록 저장하며 삭제하지 않는다
  - settings
    - 청크의 밸런싱과 관련된 작업의 설정 저장
  - version
    - 컨피그 서버가 갖고 있는 샤드 클러스터의 메타 데이터 전체에 대한 버전 정보 저장
  - lockpins
    - 컨피그 서버와의 연결을 계속해서 확인하는 데, 어떤 멤버가 언제 연결 상태를 확인했는지 저장
  - locks
    - 샤드 서버나 라우터 멤버들이 서로의 작업을 동기화하면서 처리하는 데, 작업을 동시에 시작하면 충돌이 발생할 수 있다.
    - locks 컬렉션을 활용하여 동기화하여 처리
  - changelog
    - 컨피그 서버의 메타 정보 변경을 유발한 이벤트에 관해 정보성 이력 관리
- 컨피그 서버 복제 방식
  - 메타 정보는 사용자 데이터의 일관성을 유지하기 위한 중요한 정보로 3대 이상으로 복제할 것을 권장하고 있다
  - SCCC(Sync Cluster Connection Config)
    - 각 config 서버가 완전히 독립적으로 동작, mongos(라우터)가 3대의 config 서버에 모두 직접 연결해 데이터를 동기화
    - 라우터가 데이터를 갱신할 때 3대의 config 서버 모두에 UPDATE 쿼리를 따로 보내고, 분산 트랜잭션 사용
    - 데이터 불일치, 동기화 문제, 구현 복잡성 등 여러 안정성 문제가 있어 MongoDB 3.2 이전에만 사용됩니다.
  - CSRS(Config Server as Replica Sets)
    - config 서버도 샤드 데이터처럼 레플리카 셋(복제본 세트)으로 구성하는 방식
    - 최소 3대 이상으로 구성이 필요하고, 아비터(Arbiter)나 지연 멤버는 사용할 수 없다
    - config 서버끼리 자동으로 데이터 복제를 수행하며, 장애 발생 및 일관성을 훨씬 효과적으로 보장한다
    - MongoDB 3.4 이상에서는 이 방식만 지원하며, WiredTiger 스토리지 엔진을 반드시 사용해야 한다
- 컨피그 서버 가용성과 쿼리 실행
- 라우터
- 라우터의 쿼리 분산
- 라우터 배포
- 커넥션 풀 관리
- 백업 복구 시 주의사항

#### 샤딩 알고리즘

#### 프라이머리 샤드

#### 청크 밸런싱

#### 샤딩으로 인한 제약
